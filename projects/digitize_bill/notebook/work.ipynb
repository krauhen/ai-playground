{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966daa70-3662-46a1-89d4-7842bfcd2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import glob\n",
    "import inspect\n",
    "import json\n",
    "import mlflow\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3e193-dad6-467b-ba15-c25001c982e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 0\n",
    "DEVICE = \"cpu\"\n",
    "N_EPOCHS = 100\n",
    "LR = 1e-4\n",
    "\n",
    "ROOT_PATH = \"../data\"\n",
    "FILE_NAME = os.path.join(ROOT_PATH, \"bill_detection.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96deb7-ef46-4bd2-bb12-fe97b6843074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8a01b-1034-4edd-8ae6-57f71d8df42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73006b35-d4fa-4b52-9d79-1fa83b75e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        self.root = root\n",
    "\n",
    "        with open(os.path.join(self.root, \"data.json\"), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.elements = data[\"entries\"]\n",
    "        present_elements = []\n",
    "        for element in self.elements:\n",
    "            filename = element[\"filename\"]\n",
    "            if not os.path.isfile(filename):\n",
    "                continue\n",
    "            else:\n",
    "                present_elements.append(element)\n",
    "        self.elements = present_elements\n",
    "        \n",
    "        self.visible_categories = {\"yes\": 0, \"no\": 1, \"unclear\": 2}\n",
    "        self.text_categories = {\"yes\": 0, \"no\": 1, \"unclear\": 2}\n",
    "\n",
    "        element = self.elements[0]\n",
    "        filename = element[\"filename\"]\n",
    "        img = cv2.imread(filename)\n",
    "        self.h, self.w, c = img.shape\n",
    "\n",
    "        n_train = int(len(self) * 0.8)\n",
    "\n",
    "        if train:\n",
    "            self.elements = self.elements[:n_train]\n",
    "        else:\n",
    "            self.elements = self.elements[n_train:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.elements)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        element = self.elements[idx]\n",
    "\n",
    "        label_visible = element[\"label_visible\"]\n",
    "        label_visible = np.array(self.visible_categories[label_visible], dtype=np.int64)\n",
    "        \n",
    "        label_text = element[\"label_text\"]\n",
    "        label_text = np.array(self.text_categories[label_text], dtype=np.int64)\n",
    "        \n",
    "        filename = element[\"filename\"]\n",
    "        \n",
    "        raw_coord = element[\"coord\"]\n",
    "        coord = raw_coord.replace(\")\", \"\")\n",
    "        coord = coord.replace(\"(\", \"\")\n",
    "        xl, yl, xu, yu = coord.split(\",\")\n",
    "        xl = int(xl)\n",
    "        yl = int(yl)\n",
    "        xu = int(xu)\n",
    "        yu = int(yu)\n",
    "        coords = np.array([xl, yl, xu, yu], dtype=np.float32)\n",
    "\n",
    "        if sys.platform.startswith('linux'):\n",
    "            split_char = \"/\"\n",
    "        else:\n",
    "            split_char = \"\\\\\"\n",
    "\n",
    "        fileparts = filename.split(split_char)\n",
    "        filename = fileparts[-1].split(\".\")[0] + f\"_{xl}_{yl}_{xu}_{yu}.jpg\"\n",
    "        filename = os.path.join(split_char.join(fileparts[:-1]), filename)\n",
    "        filename = filename.split(split_char)[-1]\n",
    "        filename = os.path.join(self.root, f\"patches/{filename}\")\n",
    "        \n",
    "        patch = cv2.imread(filename)\n",
    "        patch[:, :, [0, 1, 2]] = patch[:, :, [2, 1, 0]]\n",
    "        patch = torch.from_numpy(patch) / 255\n",
    "        patch = patch.permute(2, 0, 1)\n",
    "        patch = patch.to(torch.float32)\n",
    "\n",
    "        sample = dict()\n",
    "        sample[\"patch\"] = patch\n",
    "        sample[\"coords\"] = coords\n",
    "        sample[\"label_visible\"] = label_visible\n",
    "        sample[\"label_text\"] = label_text\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75801822-51fb-4ed0-8ed0-20f58c499119",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(ROOT_PATH, train=True)\n",
    "test_dataset = CustomDataset(ROOT_PATH, train=False)\n",
    "HEIGHT = train_dataset.h\n",
    "WIDTH = train_dataset.w\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=NUM_WORKERS)\n",
    "print(\"Train:\", len(train_dataset))\n",
    "print(\"Test:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59ae63-36e4-4354-b913-7f25e3fe2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEIGHT\", HEIGHT)\n",
    "print(\"WIDTH\", WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8544038-d863-4692-b849-595a96bf3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch = next(iter(train_loader))\n",
    "    coords = batch[\"coords\"]\n",
    "    patches = batch[\"patch\"]\n",
    "    print(coords.shape)\n",
    "    print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e7eec-f75b-4423-82ae-7e9eddaaef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_visible = np.array([0, 0, 0])\n",
    "count_text = np.array([0, 0, 0])\n",
    "for i, element in enumerate(train_dataset):\n",
    "    print(f\"\\r{i}|{len(train_dataset)}\", end=\"\")\n",
    "    visible = element[\"label_visible\"]\n",
    "    count_visible[visible] += 1\n",
    "    text = element[\"label_text\"]\n",
    "    count_text[text] += 1\n",
    "weights_visible = torch.from_numpy(count_visible / count_visible.sum()).to(DEVICE).to(torch.float32)\n",
    "weights_text = torch.from_numpy(count_text / count_text.sum()).to(DEVICE).to(torch.float32)\n",
    "print(count_visible)\n",
    "print(weights_visible)\n",
    "print(count_text)\n",
    "print(weights_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7866191-fd3f-41f2-bbe6-d3386a57a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    N = len(train_loader)\n",
    "    sum_loss_visible = 0\n",
    "    sum_loss_text = 0\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    for n_epoch in range(N_EPOCHS):\n",
    "        for n, batch in enumerate(train_loader):\n",
    "            with torch.autocast(device_type=DEVICE, dtype=torch.float16, enabled=False):\n",
    "                \n",
    "                patch = batch[\"patch\"].to(DEVICE)\n",
    "                label_visible = batch[\"label_visible\"].to(DEVICE)\n",
    "                label_text = batch[\"label_text\"].to(DEVICE)\n",
    "\n",
    "                visible, text = model(patch)\n",
    "                \n",
    "                loss_visible = F.cross_entropy(visible, label_visible, weight=weights_visible)\n",
    "                loss_text = F.cross_entropy(text, label_text, weight=weights_text)\n",
    "                loss = loss_visible + loss_text\n",
    "\n",
    "                mlflow.log_metric(\"train_loss\", loss, step=count, synchronous=False)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            sum_loss_visible += loss_visible.item()\n",
    "            sum_loss_text += loss_text.item()\n",
    "            sum_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "            mlflow.log_metric(\"mean_train_loss\", sum_loss / count, step=(n_epoch + 1) * (n + 1), synchronous=False)\n",
    "\n",
    "            print(f\"\\r{n_epoch + 1:03d}|{N_EPOCHS:03d}, {n + 1:04d}|{N:04d}, loss: {sum_loss / count:.05f}, loss_visible: {sum_loss_visible / count:.05f}, loss_text: {sum_loss_text / count:.05f}\", end=\"\")\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        if (n + 1) % 1 == 0:\n",
    "            print()\n",
    "            sum_loss_visible = 0\n",
    "            sum_loss_text = 0\n",
    "            sum_loss = 0\n",
    "            count = 0\n",
    "            \n",
    "        if (n + 1) % 150 == 0:\n",
    "            print(\"\\nSave...\")\n",
    "            torch.save({\"model_state_dict\": model.model.state_dict(), \n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(), \n",
    "                        \"scaler_state_dict\": scaler.state_dict()}, FILE_NAME)\n",
    "            print(\"...done!\\n\")\n",
    "            data = generate_examples(train_loader, num_new_tokens=16, verbose=False)\n",
    "            mlflow.log_dict(data, f\"example_{n_epoch + 1}_{n + 1}.json\")\n",
    "            test(n_epoch + 1, (n_epoch + 1) * (n + 1) * BATCH_ACC)\n",
    "@torch.no_grad\n",
    "def eval(loader, label):\n",
    "    acc = 0\n",
    "    count = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        patch = batch[\"patch\"].to(DEVICE)\n",
    "        label_visible = batch[\"label_visible\"]\n",
    "        label_text = batch[\"label_text\"]\n",
    "        \n",
    "        visible, text = model(patch)\n",
    "\n",
    "        visible = visible.detach().cpu().numpy().argmax(axis=1)\n",
    "        label_visible = label_visible.detach().cpu().numpy()\n",
    "\n",
    "        text = text.detach().cpu().numpy().argmax(axis=1)\n",
    "        label_text = label_text.detach().cpu().numpy()\n",
    "        \n",
    "        acc += sum([pred == gt for pred, gt in zip(visible, label_visible)]) / len(visible)\n",
    "        acc += sum([pred == gt for pred, gt in zip(text, label_text)]) / len(text)\n",
    "        count += 2\n",
    "\n",
    "        print(f\"\\r{label}, {i + 1}|{len(loader)}\", end=\"\")\n",
    "    print()\n",
    "    print(f\"Eval {label}, Accuracy: {(acc / count) * 100:0.2f}%\")\n",
    "@torch.no_grad\n",
    "def test(epoch=0, step=0):\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        img = batch[\"img\"].to(DEVICE)\n",
    "        patch = batch[\"patch\"].to(DEVICE)\n",
    "        label_visible = batch[\"label_visible\"].to(DEVICE)\n",
    "        label_text = batch[\"label_text\"].to(DEVICE)\n",
    "        \n",
    "        visible, text = model(patch)\n",
    "                    \n",
    "        loss_visible = F.cross_entropy(visible, label_visible)\n",
    "        loss_text = F.cross_entropy(text, label_text)\n",
    "        loss = loss_visible + loss_text\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "        print(f\"\\r{i + 1:04d}|{len(test_loader):04d}, loss: {sum_loss / count:.05f}\", end=\"\")\n",
    "\n",
    "    data = generate_examples(test_loader, num_new_tokens=16, verbose=False)\n",
    "    mlflow.log_dict(data, f\"example_{epoch}_{i + 1}.json\")\n",
    "    mlflow.log_metric(\"test_loss\", sum_loss / count, step=step, synchronous=False)\n",
    "    print()\n",
    "    print()\n",
    "@torch.no_grad\n",
    "def test_model_config():\n",
    "    batch = next(iter(train_loader))\n",
    "    patch = batch[\"patch\"].to(DEVICE)\n",
    "    label_visible = batch[\"label_visible\"].to(DEVICE)\n",
    "    label_text = batch[\"label_text\"].to(DEVICE)\n",
    "    print(patch.shape)\n",
    "    print(label_visible.shape)\n",
    "    print(label_text.shape)\n",
    "    visible, text = model(patch)\n",
    "    print(visible.shape)\n",
    "    print(text.shape)\n",
    "    loss_visible = F.cross_entropy(visible, label_visible, weight=weights_visible)\n",
    "    print(loss_visible)\n",
    "    loss_text = F.cross_entropy(text, label_text, weight=weights_text)\n",
    "    print(loss_text)\n",
    "    loss = loss_visible + loss_text\n",
    "    print(loss)\n",
    "    print()\n",
    "@torch.no_grad\n",
    "def generate_maps(loader):\n",
    "    with open(os.path.join(ROOT_PATH, \"data.json\"), \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    elements = data[\"entries\"]\n",
    "    while True:\n",
    "        idx = np.random.randint(0, len(elements))\n",
    "        element = elements[idx]\n",
    "        filename = element[\"filename\"]\n",
    "        if os.path.isfile(filename):\n",
    "            break\n",
    "\n",
    "    raw_img = cv2.imread(filename)\n",
    "    H, W, c = raw_img.shape\n",
    "    raw_img[:, :, [0, 1, 2]] = raw_img[:, :, [2, 1, 0]]\n",
    "    img = torch.from_numpy(raw_img) / 255\n",
    "    img = img.permute(2, 0, 1)\n",
    "\n",
    "    ph, pw = 256, 256\n",
    "\n",
    "    map_visible = np.zeros([H, W, 3])\n",
    "    map_text = np.zeros([H, W, 3])\n",
    "    for xl in range(0, W - pw, pw):\n",
    "        for yl in range(0, H - ph, ph):\n",
    "            print(f\"\\r{xl}, {yl}\", end=\"\")\n",
    "            xu = xl + pw\n",
    "            yu = yl + ph\n",
    "            patch = img[:, yl:yu, xl:xu]\n",
    "            patch = patch[None, :]\n",
    "            patch = patch.to(DEVICE)\n",
    "            \n",
    "            visible, text = model(patch)\n",
    "        \n",
    "            visible = visible.detach().cpu().numpy().argmax(axis=1)[0]\n",
    "            text = text.detach().cpu().numpy().argmax(axis=1)[0]\n",
    "        \n",
    "            map_visible[yl:yu, xl:xu, visible] += 1\n",
    "            map_text[yl:yu, xl:xu, text] += 1\n",
    "\n",
    "    map_visible = map_visible.argmax(axis=-1)\n",
    "    \n",
    "    map_visible_yes = map_visible.copy()\n",
    "    map_visible_yes[map_visible_yes == 0] = 1.0\n",
    "    map_visible_yes = map_visible_yes[:, :, None]\n",
    "\n",
    "    map_visible_no = map_visible.copy()\n",
    "    map_visible_no[map_visible_no == 1] = 1.0\n",
    "    map_visible_no = map_visible_no[:, :, None]\n",
    "\n",
    "    map_visible_unclear = map_visible.copy()\n",
    "    map_visible_unclear[map_visible_unclear == 2] = 1.0\n",
    "    map_visible_unclear = map_visible_unclear[:, :, None]\n",
    "\n",
    "    map_visible = np.concatenate([map_visible_yes, map_visible_no, map_visible_unclear], axis=-1)\n",
    "    map_visible = np.asarray(map_visible, dtype=np.float64)\n",
    "    \n",
    "    map_text = map_text.argmax(axis=-1)\n",
    "    \n",
    "    map_text_yes = map_text.copy()\n",
    "    map_text_yes[map_text_yes == 0] = 1.0\n",
    "    map_text_yes = map_text_yes[:, :, None]\n",
    "\n",
    "    map_text_no = map_text.copy()\n",
    "    map_text_no[map_text_no == 1] = 1.0\n",
    "    map_text_no = map_text_no[:, :, None]\n",
    "\n",
    "    map_text_unclear = map_text.copy()\n",
    "    map_text_unclear[map_text_unclear == 2] = 1.0\n",
    "    map_text_unclear = map_text_unclear[:, :, None]\n",
    "\n",
    "    map_text = np.concatenate([map_text_yes, map_text_no, map_text_unclear], axis=-1)\n",
    "    map_text = np.asarray(map_text, dtype=np.float64)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "    axes[0].imshow(raw_img)\n",
    "    axes[0].set_title(\"img\")\n",
    "    axes[1].imshow(map_visible)\n",
    "    axes[1].set_title(\"visible\")\n",
    "    axes[2].imshow(map_text)\n",
    "    axes[2].set_title(\"text\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672aa99-d871-46db-ab90-d8b8e4aa2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.AvgPool2d(2, 2),\n",
    "                                     nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.AvgPool2d(2, 2),\n",
    "                                     nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU())\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.out = nn.Sequential(nn.Linear(1024, 128),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(128, 128),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(128, 3 + 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        latent = x.view(x.shape[0], -1)\n",
    "        latent = self.dropout(latent)\n",
    "        x = self.out(latent)\n",
    "        visible = F.softmax(x[:, :3], dim=-1)\n",
    "        text = F.softmax(x[:, 3:], dim=-1)\n",
    "        \n",
    "        return visible, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da4c78-9a08-45aa-985e-a2a59aa7608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE)\n",
    "\n",
    "if checkpoint is not None:\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "if checkpoint is not None:\n",
    "    scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "if checkpoint is not None:\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "n_model_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {n_model_parameters:,}\".replace(\",\", \".\"))\n",
    "print()\n",
    "\n",
    "test_model_config()\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")\n",
    "_ = mlflow.set_experiment(f\"Bill images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6fd79-f98e-4d5d-a0c7-7fc06fe94e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = None\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b90bf3-ce76-4825-9764-d88afd5c234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(train_loader, \"TRAIN\")\n",
    "eval(test_loader, \"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6124242c-8806-4988-afe2-c7e50828fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict(), \n",
    "            \"optimizer_state_dict\": optimizer.state_dict(), \n",
    "            \"scaler_state_dict\": scaler.state_dict()}, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341056f5-3d1e-4a59-ad3d-f0a4237292df",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_maps(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b2826-68d3-41b3-8106-da6dcad21632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
