{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966daa70-3662-46a1-89d4-7842bfcd2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import glob\n",
    "import inspect\n",
    "import json\n",
    "import mlflow\n",
    "import tiktoken\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3e193-dad6-467b-ba15-c25001c982e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "BATCH_ACC = 1\n",
    "NUM_WORKERS = 16\n",
    "DEVICE = \"cuda\"\n",
    "N_EPOCHS = 100\n",
    "LR = 1e-4\n",
    "\n",
    "ROOT_PATH = \"/home/henning/repos/ai-playground/projects/label_images\"\n",
    "FILE_NAME = f\"/mnt/data/checkpoints/bill_detection.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96deb7-ef46-4bd2-bb12-fe97b6843074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8a01b-1034-4edd-8ae6-57f71d8df42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73006b35-d4fa-4b52-9d79-1fa83b75e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        self.root = root\n",
    "\n",
    "        with open(os.path.join(self.root, \"data.json\"), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.elements = data[\"entries\"]\n",
    "        self.visible_categories = {\"yes\": 0, \"no\": 1, \"unclear\": 2}\n",
    "        self.text_categories = {\"yes\": 0, \"no\": 1, \"unclear\": 2}\n",
    "\n",
    "        n_train = int(len(self) * 0.8)\n",
    "\n",
    "        if train:\n",
    "            self.elements = self.elements[:n_train]\n",
    "        else:\n",
    "            self.elements = self.elements[n_train:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.elements)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        element = self.elements[idx]\n",
    "\n",
    "        label_visible = element[\"label_visible\"]\n",
    "        label_visible = np.array(self.visible_categories[label_visible])\n",
    "        \n",
    "        label_text = element[\"label_text\"]\n",
    "        label_text = np.array(self.text_categories[label_text])\n",
    "        \n",
    "        filename = element[\"filename\"]\n",
    "        \n",
    "        coord = element[\"coord\"]\n",
    "        coord = coord.replace(\")\", \"\")\n",
    "        coord = coord.replace(\"(\", \"\")\n",
    "        xl, yl, xu, yu = coord.split(\",\")\n",
    "        xl = int(xl)\n",
    "        yl = int(yl)\n",
    "        xu = int(xu)\n",
    "        yu = int(yu)\n",
    "\n",
    "        img = cv2.imread(filename)\n",
    "        h, w, c = img.shape\n",
    "        img[:, :, [0, 1, 2]] = img[:, :, [2, 1, 0]]\n",
    "        img = torch.from_numpy(img) / 255\n",
    "        img = img.permute(2, 0, 1)\n",
    "\n",
    "        if w > h:\n",
    "            img = img.permute(0, 2, 1)\n",
    "\n",
    "        patch = img[:, yl:yu, xl:xu]\n",
    "\n",
    "        sample = dict()\n",
    "        sample[\"img\"] = img\n",
    "        sample[\"patch\"] = patch\n",
    "        sample[\"label_visible\"] = label_visible\n",
    "        sample[\"label_text\"] = label_text\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75801822-51fb-4ed0-8ed0-20f58c499119",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(ROOT_PATH, train=True)\n",
    "test_dataset = CustomDataset(ROOT_PATH, train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=NUM_WORKERS)\n",
    "print(\"Train:\", len(train_dataset))\n",
    "print(\"Test:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8b6f1-c200-4186-a4ab-8a13e78721b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "element = train_dataset[0]\n",
    "print(element[\"img\"].shape)\n",
    "print(element[\"patch\"].shape)\n",
    "print(element[\"label_visible\"].shape)\n",
    "print(element[\"label_text\"].shape)\n",
    "print()\n",
    "batch = next(iter(train_loader))\n",
    "print(batch[\"img\"].shape)\n",
    "print(batch[\"patch\"].shape)\n",
    "print(batch[\"label_visible\"].shape)\n",
    "print(batch[\"label_text\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672aa99-d871-46db-ab90-d8b8e4aa2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.AvgPool2d(2, 2),\n",
    "                                     nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.AvgPool2d(2, 2),\n",
    "                                     nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU())\n",
    "\n",
    "        self.latent_visible = nn.Sequential(nn.Linear(1024, 128),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(128, 3),\n",
    "                                            nn.Softmax(dim=-1))\n",
    "\n",
    "        self.latent_text = nn.Sequential(nn.Linear(1024, 128),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(128, 3),\n",
    "                                         nn.Softmax(dim=-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        latent = x.view(x.shape[0], -1)\n",
    "        visible = self.latent_visible(latent)\n",
    "        text = self.latent_text(latent)\n",
    "        \n",
    "        return visible, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da4c78-9a08-45aa-985e-a2a59aa7608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE)\n",
    "\n",
    "if checkpoint is not None:\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "if checkpoint is not None:\n",
    "    scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "if checkpoint is not None:\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "n_model_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {n_model_parameters:,}\".replace(\",\", \".\"))\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(train_loader))\n",
    "    img = batch[\"img\"].to(DEVICE)\n",
    "    patch = batch[\"patch\"].to(DEVICE)\n",
    "    label_visible = batch[\"label_visible\"].to(DEVICE)\n",
    "    label_text = batch[\"label_text\"].to(DEVICE)\n",
    "    print(img.shape)\n",
    "    print(patch.shape)\n",
    "    print(label_visible.shape)\n",
    "    print(label_text.shape)\n",
    "    visible, text = model(patch)\n",
    "    print(visible.shape)\n",
    "    print(text.shape)\n",
    "    loss_visible = F.cross_entropy(visible, label_visible)\n",
    "    print(loss_visible)\n",
    "    loss_text = F.cross_entropy(text, label_text)\n",
    "    print(loss_text)\n",
    "    loss = loss_visible + loss_text\n",
    "    print(loss)\n",
    "    print()\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")\n",
    "_ = mlflow.set_experiment(f\"Bill images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bda81-27bd-4588-8eed-e71972fe74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(loader, num_new_tokens=16, verbose=False):\n",
    "    data = dict()\n",
    "    return data\n",
    "def train():\n",
    "    iterator = iter(train_loader)\n",
    "    N = len(train_loader) // BATCH_SIZE\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    for n_epoch in range(N_EPOCHS):\n",
    "        for n in range(N):\n",
    "            for b in range(BATCH_ACC):\n",
    "                with torch.autocast(device_type=DEVICE, dtype=torch.float16, enabled=True):\n",
    "                    batch = next(iterator)\n",
    "                    \n",
    "                    img = batch[\"img\"].to(DEVICE)\n",
    "                    patch = batch[\"patch\"].to(DEVICE)\n",
    "                    label_visible = batch[\"label_visible\"].to(DEVICE)\n",
    "                    label_text = batch[\"label_text\"].to(DEVICE)\n",
    "                    \n",
    "                    visible, text = model(patch)\n",
    "                    \n",
    "                    loss_visible = F.cross_entropy(visible, label_visible)\n",
    "                    loss_text = F.cross_entropy(text, label_text)\n",
    "                    loss = loss_visible + loss_text\n",
    "    \n",
    "                    mlflow.log_metric(\"train_loss\", loss, step=count, synchronous=False)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                sum_loss += loss.item()\n",
    "                count += 1\n",
    "\n",
    "                mlflow.log_metric(\"mean_train_loss\", sum_loss / count, step=(n_epoch + 1) * (n + 1) * BATCH_ACC, synchronous=False)\n",
    "\n",
    "                print(f\"\\r{n_epoch + 1:03d}|{N_EPOCHS:03d}, {n + 1:04d}|{N:04d}, {b + 1:03d}|{BATCH_ACC:03d}, loss: {sum_loss / count:.05f}\", end=\"\")\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "            if (n + 1) % 25 == 0:\n",
    "                print()\n",
    "                sum_loss = 0\n",
    "                count = 0\n",
    "                \n",
    "            if (n + 1) % 150 == 0:\n",
    "                print(\"\\nSave...\")\n",
    "                torch.save({\"model_state_dict\": model.model.state_dict(), \n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(), \n",
    "                            \"scaler_state_dict\": scaler.state_dict()}, FILE_NAME)\n",
    "                print(\"...done!\\n\")\n",
    "                data = generate_examples(train_loader, num_new_tokens=16, verbose=False)\n",
    "                mlflow.log_dict(data, f\"example_{n_epoch + 1}_{n + 1}.json\")\n",
    "                test(n_epoch + 1, (n_epoch + 1) * (n + 1) * BATCH_ACC)\n",
    "@torch.no_grad\n",
    "def test(epoch=0, step=0):\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        img = batch[\"img\"].to(DEVICE)\n",
    "        patch = batch[\"patch\"].to(DEVICE)\n",
    "        label_visible = batch[\"label_visible\"].to(DEVICE)\n",
    "        label_text = batch[\"label_text\"].to(DEVICE)\n",
    "        \n",
    "        visible, text = model(patch)\n",
    "                    \n",
    "        loss_visible = F.cross_entropy(visible, label_visible)\n",
    "        loss_text = F.cross_entropy(text, label_text)\n",
    "        loss = loss_visible + loss_text\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "        print(f\"\\r{i + 1:04d}|{len(test_loader):04d}, loss: {sum_loss / count:.05f}\", end=\"\")\n",
    "\n",
    "    data = generate_examples(test_loader, num_new_tokens=16, verbose=False)\n",
    "    mlflow.log_dict(data, f\"example_{epoch}_{i + 1}.json\")\n",
    "    mlflow.log_metric(\"test_loss\", sum_loss / count, step=step, synchronous=False)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6fd79-f98e-4d5d-a0c7-7fc06fe94e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = None\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6124242c-8806-4988-afe2-c7e50828fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.model.state_dict(), \n",
    "            \"optimizer_state_dict\": optimizer.state_dict(), \n",
    "            \"scaler_state_dict\": scaler.state_dict()}, FILE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
