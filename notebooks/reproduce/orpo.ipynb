{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50798a82-ef70-4e42-86e9-ddc62108f1ee",
   "metadata": {},
   "source": [
    "# Source\n",
    "# https://huggingface.co/blog/mlabonne/orpo-llama-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b020c7-e1ea-4d8a-9b63-f65b3af7ab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henning/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21797f14-0d23-476e-91e7-57df9b0f06a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc3f334-1ddb-4330-8c10-0ad956f0096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089e047b-6f00-44bd-9465-6f21ce72aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flash attention\n",
    "attn_implementation = \"flash_attention_2\"\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# Model\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "new_model = \"OrpoLlama-3-8B\"\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39403438-adf6-46a8-b7a8-a1b04f3726ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model and Tokenizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "230a175e-90b4-4225-8d96-2e7577172953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    token=access_token)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    token=access_token\n",
    ")\n",
    "\n",
    "# Prepare model embedding layer for tokenizer dictionary\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "# Prepare/Wrap model for quantization\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec70b8-5407-448b-aaa7-211da92804a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a5130a7-406d-480a-b611-21445a06cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "#dataset = dataset.select(range(3930))\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    return row\n",
    "\n",
    "# Process entries when called to fit the correct template\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= os.cpu_count(),\n",
    ")\n",
    "dataset = dataset.train_test_split(test_size=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b673197-44db-4a07-9d33-dc97f5537f59",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31d5c7-1019-4fcb-aede-aa8ace169d93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15fbe89e-a54c-4791-ab9e-8d86a2e1bee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39663/39663 [01:26<00:00, 457.76 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 404.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=8e-6,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    output_dir=\"./results/\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ccc73-ccac-4e1f-9654-2af82ae2636a",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd4ea00c-b425-42d0-bf2e-82ee636105b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henning/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='495800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    80/495800 1:40:12 < 10614:55:30, 0.01 it/s, Epoch 0.02/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Nll Loss</th>\n",
       "      <th>Log Odds Ratio</th>\n",
       "      <th>Log Odds Chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.433800</td>\n",
       "      <td>4.921563</td>\n",
       "      <td>46.908000</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>0.426000</td>\n",
       "      <td>-0.147043</td>\n",
       "      <td>-0.143011</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>-1.430109</td>\n",
       "      <td>-1.470430</td>\n",
       "      <td>-1.451707</td>\n",
       "      <td>-1.470076</td>\n",
       "      <td>4.844595</td>\n",
       "      <td>-0.769671</td>\n",
       "      <td>-0.047337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.701000</td>\n",
       "      <td>4.866637</td>\n",
       "      <td>50.502800</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>-0.147021</td>\n",
       "      <td>-0.143002</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.004019</td>\n",
       "      <td>-1.430023</td>\n",
       "      <td>-1.470213</td>\n",
       "      <td>-1.462631</td>\n",
       "      <td>-1.480098</td>\n",
       "      <td>4.789681</td>\n",
       "      <td>-0.769555</td>\n",
       "      <td>-0.047185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.098400</td>\n",
       "      <td>4.756692</td>\n",
       "      <td>51.655400</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>-0.146952</td>\n",
       "      <td>-0.142946</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.004005</td>\n",
       "      <td>-1.429465</td>\n",
       "      <td>-1.469516</td>\n",
       "      <td>-1.484007</td>\n",
       "      <td>-1.499783</td>\n",
       "      <td>4.679748</td>\n",
       "      <td>-0.769437</td>\n",
       "      <td>-0.047007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.963400</td>\n",
       "      <td>4.592246</td>\n",
       "      <td>51.382400</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>-0.146874</td>\n",
       "      <td>-0.142866</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.004009</td>\n",
       "      <td>-1.428655</td>\n",
       "      <td>-1.468741</td>\n",
       "      <td>-1.514790</td>\n",
       "      <td>-1.528304</td>\n",
       "      <td>4.515296</td>\n",
       "      <td>-0.769487</td>\n",
       "      <td>-0.047082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.519800</td>\n",
       "      <td>4.374752</td>\n",
       "      <td>52.459300</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>-0.146700</td>\n",
       "      <td>-0.142709</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.003991</td>\n",
       "      <td>-1.427088</td>\n",
       "      <td>-1.467002</td>\n",
       "      <td>-1.553582</td>\n",
       "      <td>-1.564027</td>\n",
       "      <td>4.297818</td>\n",
       "      <td>-0.769332</td>\n",
       "      <td>-0.046898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.478600</td>\n",
       "      <td>4.107249</td>\n",
       "      <td>52.060200</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>-0.146322</td>\n",
       "      <td>-0.142386</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003937</td>\n",
       "      <td>-1.423856</td>\n",
       "      <td>-1.463223</td>\n",
       "      <td>-1.598696</td>\n",
       "      <td>-1.605860</td>\n",
       "      <td>4.030358</td>\n",
       "      <td>-0.768914</td>\n",
       "      <td>-0.046296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.418200</td>\n",
       "      <td>3.798208</td>\n",
       "      <td>51.651300</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>-0.146070</td>\n",
       "      <td>-0.142149</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003921</td>\n",
       "      <td>-1.421494</td>\n",
       "      <td>-1.460704</td>\n",
       "      <td>-1.641586</td>\n",
       "      <td>-1.645468</td>\n",
       "      <td>3.721333</td>\n",
       "      <td>-0.768740</td>\n",
       "      <td>-0.046180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.943100</td>\n",
       "      <td>3.448894</td>\n",
       "      <td>50.948600</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>-0.145639</td>\n",
       "      <td>-0.141770</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003869</td>\n",
       "      <td>-1.417701</td>\n",
       "      <td>-1.456388</td>\n",
       "      <td>-1.683240</td>\n",
       "      <td>-1.684193</td>\n",
       "      <td>3.372061</td>\n",
       "      <td>-0.768320</td>\n",
       "      <td>-0.045629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.624900</td>\n",
       "      <td>3.063257</td>\n",
       "      <td>50.746700</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>-0.145308</td>\n",
       "      <td>-0.141453</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003855</td>\n",
       "      <td>-1.414526</td>\n",
       "      <td>-1.453077</td>\n",
       "      <td>-1.717063</td>\n",
       "      <td>-1.715632</td>\n",
       "      <td>2.986442</td>\n",
       "      <td>-0.768151</td>\n",
       "      <td>-0.045555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.263900</td>\n",
       "      <td>2.641032</td>\n",
       "      <td>53.096400</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>-0.144900</td>\n",
       "      <td>-0.141067</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003833</td>\n",
       "      <td>-1.410672</td>\n",
       "      <td>-1.448999</td>\n",
       "      <td>-1.747823</td>\n",
       "      <td>-1.744120</td>\n",
       "      <td>2.564237</td>\n",
       "      <td>-0.767945</td>\n",
       "      <td>-0.045436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.360600</td>\n",
       "      <td>2.187634</td>\n",
       "      <td>48.875200</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>-0.144375</td>\n",
       "      <td>-0.140571</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003804</td>\n",
       "      <td>-1.405705</td>\n",
       "      <td>-1.443746</td>\n",
       "      <td>-1.782804</td>\n",
       "      <td>-1.776845</td>\n",
       "      <td>2.110872</td>\n",
       "      <td>-0.767615</td>\n",
       "      <td>-0.045190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.258600</td>\n",
       "      <td>1.768476</td>\n",
       "      <td>49.063300</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>-0.143973</td>\n",
       "      <td>-0.140188</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003785</td>\n",
       "      <td>-1.401877</td>\n",
       "      <td>-1.439726</td>\n",
       "      <td>-1.818012</td>\n",
       "      <td>-1.809412</td>\n",
       "      <td>1.691738</td>\n",
       "      <td>-0.767388</td>\n",
       "      <td>-0.045091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.730800</td>\n",
       "      <td>1.435313</td>\n",
       "      <td>49.588900</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>-0.143439</td>\n",
       "      <td>-0.139694</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003745</td>\n",
       "      <td>-1.396938</td>\n",
       "      <td>-1.434389</td>\n",
       "      <td>-1.858760</td>\n",
       "      <td>-1.847594</td>\n",
       "      <td>1.358614</td>\n",
       "      <td>-0.766989</td>\n",
       "      <td>-0.044706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.380200</td>\n",
       "      <td>1.261190</td>\n",
       "      <td>49.696400</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.402000</td>\n",
       "      <td>-0.142977</td>\n",
       "      <td>-0.139251</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.003725</td>\n",
       "      <td>-1.392512</td>\n",
       "      <td>-1.429766</td>\n",
       "      <td>-1.898150</td>\n",
       "      <td>-1.884171</td>\n",
       "      <td>1.184515</td>\n",
       "      <td>-0.766752</td>\n",
       "      <td>-0.044615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.197700</td>\n",
       "      <td>1.205609</td>\n",
       "      <td>51.296100</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>-0.142436</td>\n",
       "      <td>-0.138746</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003690</td>\n",
       "      <td>-1.387460</td>\n",
       "      <td>-1.424358</td>\n",
       "      <td>-1.934311</td>\n",
       "      <td>-1.917913</td>\n",
       "      <td>1.128968</td>\n",
       "      <td>-0.766403</td>\n",
       "      <td>-0.044304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.191478</td>\n",
       "      <td>50.463200</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>-0.141987</td>\n",
       "      <td>-0.138310</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003677</td>\n",
       "      <td>-1.383101</td>\n",
       "      <td>-1.419870</td>\n",
       "      <td>-1.958086</td>\n",
       "      <td>-1.940006</td>\n",
       "      <td>1.114863</td>\n",
       "      <td>-0.766156</td>\n",
       "      <td>-0.044286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.306300</td>\n",
       "      <td>1.185596</td>\n",
       "      <td>49.013600</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>-0.141401</td>\n",
       "      <td>-0.137768</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003632</td>\n",
       "      <td>-1.377685</td>\n",
       "      <td>-1.414005</td>\n",
       "      <td>-1.974951</td>\n",
       "      <td>-1.956035</td>\n",
       "      <td>1.109030</td>\n",
       "      <td>-0.765652</td>\n",
       "      <td>-0.043851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.524000</td>\n",
       "      <td>1.182113</td>\n",
       "      <td>49.071300</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>-0.140874</td>\n",
       "      <td>-0.137249</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003625</td>\n",
       "      <td>-1.372486</td>\n",
       "      <td>-1.408738</td>\n",
       "      <td>-1.984921</td>\n",
       "      <td>-1.965242</td>\n",
       "      <td>1.105570</td>\n",
       "      <td>-0.765430</td>\n",
       "      <td>-0.043914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.164300</td>\n",
       "      <td>1.178094</td>\n",
       "      <td>50.211200</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>-0.140220</td>\n",
       "      <td>-0.136627</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>-1.366269</td>\n",
       "      <td>-1.402198</td>\n",
       "      <td>-1.991738</td>\n",
       "      <td>-1.971806</td>\n",
       "      <td>1.101595</td>\n",
       "      <td>-0.764985</td>\n",
       "      <td>-0.043638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.149000</td>\n",
       "      <td>1.173773</td>\n",
       "      <td>49.473000</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>-0.139502</td>\n",
       "      <td>-0.135941</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>-1.359411</td>\n",
       "      <td>-1.395023</td>\n",
       "      <td>-1.995788</td>\n",
       "      <td>-1.975809</td>\n",
       "      <td>1.097321</td>\n",
       "      <td>-0.764518</td>\n",
       "      <td>-0.043416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.152100</td>\n",
       "      <td>1.169475</td>\n",
       "      <td>48.996500</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>-0.138755</td>\n",
       "      <td>-0.135243</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003512</td>\n",
       "      <td>-1.352434</td>\n",
       "      <td>-1.387549</td>\n",
       "      <td>-1.997152</td>\n",
       "      <td>-1.977282</td>\n",
       "      <td>1.093077</td>\n",
       "      <td>-0.763974</td>\n",
       "      <td>-0.042994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.412800</td>\n",
       "      <td>1.165022</td>\n",
       "      <td>49.466000</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>-0.137961</td>\n",
       "      <td>-0.134506</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>-0.003455</td>\n",
       "      <td>-1.345061</td>\n",
       "      <td>-1.379609</td>\n",
       "      <td>-1.997351</td>\n",
       "      <td>-1.977700</td>\n",
       "      <td>1.088690</td>\n",
       "      <td>-0.763322</td>\n",
       "      <td>-0.042521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.208200</td>\n",
       "      <td>1.160359</td>\n",
       "      <td>49.535300</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>-0.137149</td>\n",
       "      <td>-0.133748</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>-0.003401</td>\n",
       "      <td>-1.337478</td>\n",
       "      <td>-1.371493</td>\n",
       "      <td>-1.995947</td>\n",
       "      <td>-1.976762</td>\n",
       "      <td>1.084088</td>\n",
       "      <td>-0.762709</td>\n",
       "      <td>-0.042035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.134600</td>\n",
       "      <td>1.155653</td>\n",
       "      <td>49.083100</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>-0.136320</td>\n",
       "      <td>-0.132977</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>-0.003343</td>\n",
       "      <td>-1.329766</td>\n",
       "      <td>-1.363199</td>\n",
       "      <td>-1.994189</td>\n",
       "      <td>-1.975257</td>\n",
       "      <td>1.079447</td>\n",
       "      <td>-0.762062</td>\n",
       "      <td>-0.041528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.094700</td>\n",
       "      <td>1.150848</td>\n",
       "      <td>48.856000</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>-0.135489</td>\n",
       "      <td>-0.132204</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.003285</td>\n",
       "      <td>-1.322041</td>\n",
       "      <td>-1.354892</td>\n",
       "      <td>-1.992481</td>\n",
       "      <td>-1.974070</td>\n",
       "      <td>1.074705</td>\n",
       "      <td>-0.761433</td>\n",
       "      <td>-0.040974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.394500</td>\n",
       "      <td>1.146135</td>\n",
       "      <td>49.320700</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>-0.134639</td>\n",
       "      <td>-0.131434</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>-0.003205</td>\n",
       "      <td>-1.314336</td>\n",
       "      <td>-1.346390</td>\n",
       "      <td>-1.990166</td>\n",
       "      <td>-1.972233</td>\n",
       "      <td>1.070069</td>\n",
       "      <td>-0.760659</td>\n",
       "      <td>-0.040213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.212200</td>\n",
       "      <td>1.140787</td>\n",
       "      <td>49.455200</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>-0.133743</td>\n",
       "      <td>-0.130633</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>-0.003111</td>\n",
       "      <td>-1.306325</td>\n",
       "      <td>-1.337435</td>\n",
       "      <td>-1.989917</td>\n",
       "      <td>-1.972633</td>\n",
       "      <td>1.064810</td>\n",
       "      <td>-0.759773</td>\n",
       "      <td>-0.039172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.029400</td>\n",
       "      <td>1.135908</td>\n",
       "      <td>49.345400</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>-0.132877</td>\n",
       "      <td>-0.129825</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>-0.003052</td>\n",
       "      <td>-1.298254</td>\n",
       "      <td>-1.328769</td>\n",
       "      <td>-1.987632</td>\n",
       "      <td>-1.970977</td>\n",
       "      <td>1.059991</td>\n",
       "      <td>-0.759175</td>\n",
       "      <td>-0.038662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.172900</td>\n",
       "      <td>1.130853</td>\n",
       "      <td>49.491200</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>-0.132002</td>\n",
       "      <td>-0.129062</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>-0.002940</td>\n",
       "      <td>-1.290618</td>\n",
       "      <td>-1.320016</td>\n",
       "      <td>-1.987390</td>\n",
       "      <td>-1.971417</td>\n",
       "      <td>1.055032</td>\n",
       "      <td>-0.758212</td>\n",
       "      <td>-0.037410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.183200</td>\n",
       "      <td>1.125990</td>\n",
       "      <td>49.700700</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.402000</td>\n",
       "      <td>-0.131118</td>\n",
       "      <td>-0.128276</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.002842</td>\n",
       "      <td>-1.282758</td>\n",
       "      <td>-1.311182</td>\n",
       "      <td>-1.985848</td>\n",
       "      <td>-1.970376</td>\n",
       "      <td>1.050254</td>\n",
       "      <td>-0.757365</td>\n",
       "      <td>-0.036368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.076400</td>\n",
       "      <td>1.120926</td>\n",
       "      <td>49.597300</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>-0.130247</td>\n",
       "      <td>-0.127502</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.002746</td>\n",
       "      <td>-1.275015</td>\n",
       "      <td>-1.302473</td>\n",
       "      <td>-1.985295</td>\n",
       "      <td>-1.970748</td>\n",
       "      <td>1.045273</td>\n",
       "      <td>-0.756524</td>\n",
       "      <td>-0.035277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.050200</td>\n",
       "      <td>1.115991</td>\n",
       "      <td>49.497400</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>-0.129351</td>\n",
       "      <td>-0.126685</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>-1.266848</td>\n",
       "      <td>-1.293513</td>\n",
       "      <td>-1.983319</td>\n",
       "      <td>-1.969272</td>\n",
       "      <td>1.040403</td>\n",
       "      <td>-0.755879</td>\n",
       "      <td>-0.034557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.266400</td>\n",
       "      <td>1.110887</td>\n",
       "      <td>49.239300</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>-0.128421</td>\n",
       "      <td>-0.125853</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.002568</td>\n",
       "      <td>-1.258532</td>\n",
       "      <td>-1.284209</td>\n",
       "      <td>-1.981175</td>\n",
       "      <td>-1.967853</td>\n",
       "      <td>1.035382</td>\n",
       "      <td>-0.755053</td>\n",
       "      <td>-0.033514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>1.105811</td>\n",
       "      <td>48.532500</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>-0.127483</td>\n",
       "      <td>-0.125043</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.002440</td>\n",
       "      <td>-1.250430</td>\n",
       "      <td>-1.274833</td>\n",
       "      <td>-1.980837</td>\n",
       "      <td>-1.968254</td>\n",
       "      <td>1.030407</td>\n",
       "      <td>-0.754039</td>\n",
       "      <td>-0.032056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.451700</td>\n",
       "      <td>1.100637</td>\n",
       "      <td>49.550100</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>-0.126530</td>\n",
       "      <td>-0.124209</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.002321</td>\n",
       "      <td>-1.242091</td>\n",
       "      <td>-1.265299</td>\n",
       "      <td>-1.980476</td>\n",
       "      <td>-1.968616</td>\n",
       "      <td>1.025326</td>\n",
       "      <td>-0.753113</td>\n",
       "      <td>-0.030740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.001000</td>\n",
       "      <td>1.095631</td>\n",
       "      <td>48.924600</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>-0.125575</td>\n",
       "      <td>-0.123370</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.002205</td>\n",
       "      <td>-1.233703</td>\n",
       "      <td>-1.255753</td>\n",
       "      <td>-1.979660</td>\n",
       "      <td>-1.968405</td>\n",
       "      <td>1.020410</td>\n",
       "      <td>-0.752213</td>\n",
       "      <td>-0.029480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.119000</td>\n",
       "      <td>1.090530</td>\n",
       "      <td>49.310500</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>-0.124614</td>\n",
       "      <td>-0.122541</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.002073</td>\n",
       "      <td>-1.225411</td>\n",
       "      <td>-1.246145</td>\n",
       "      <td>-1.979015</td>\n",
       "      <td>-1.968155</td>\n",
       "      <td>1.015404</td>\n",
       "      <td>-0.751256</td>\n",
       "      <td>-0.027999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.806400</td>\n",
       "      <td>1.085596</td>\n",
       "      <td>49.009400</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>-0.123691</td>\n",
       "      <td>-0.121767</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.001924</td>\n",
       "      <td>-1.217674</td>\n",
       "      <td>-1.236915</td>\n",
       "      <td>-1.978879</td>\n",
       "      <td>-1.968591</td>\n",
       "      <td>1.010579</td>\n",
       "      <td>-0.750174</td>\n",
       "      <td>-0.026235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.080749</td>\n",
       "      <td>50.167600</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.399000</td>\n",
       "      <td>-0.122781</td>\n",
       "      <td>-0.121009</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.001772</td>\n",
       "      <td>-1.210091</td>\n",
       "      <td>-1.227814</td>\n",
       "      <td>-1.978160</td>\n",
       "      <td>-1.968529</td>\n",
       "      <td>1.005841</td>\n",
       "      <td>-0.749079</td>\n",
       "      <td>-0.024391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.936400</td>\n",
       "      <td>1.076078</td>\n",
       "      <td>50.022100</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-0.121918</td>\n",
       "      <td>-0.120303</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.001615</td>\n",
       "      <td>-1.203035</td>\n",
       "      <td>-1.219182</td>\n",
       "      <td>-1.977391</td>\n",
       "      <td>-1.968204</td>\n",
       "      <td>1.001281</td>\n",
       "      <td>-0.747968</td>\n",
       "      <td>-0.022419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.273200</td>\n",
       "      <td>1.071705</td>\n",
       "      <td>49.628400</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>-0.121125</td>\n",
       "      <td>-0.119676</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.001449</td>\n",
       "      <td>-1.196762</td>\n",
       "      <td>-1.211253</td>\n",
       "      <td>-1.977929</td>\n",
       "      <td>-1.969392</td>\n",
       "      <td>0.997023</td>\n",
       "      <td>-0.746817</td>\n",
       "      <td>-0.020248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.110900</td>\n",
       "      <td>1.067944</td>\n",
       "      <td>50.474400</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>-0.120419</td>\n",
       "      <td>-0.119159</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.001260</td>\n",
       "      <td>-1.191587</td>\n",
       "      <td>-1.204191</td>\n",
       "      <td>-1.976919</td>\n",
       "      <td>-1.968637</td>\n",
       "      <td>0.993392</td>\n",
       "      <td>-0.745517</td>\n",
       "      <td>-0.017758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.040900</td>\n",
       "      <td>1.064141</td>\n",
       "      <td>49.323700</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>-0.119749</td>\n",
       "      <td>-0.118687</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.001062</td>\n",
       "      <td>-1.186871</td>\n",
       "      <td>-1.197489</td>\n",
       "      <td>-1.977239</td>\n",
       "      <td>-1.969424</td>\n",
       "      <td>0.989725</td>\n",
       "      <td>-0.744158</td>\n",
       "      <td>-0.015087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.958500</td>\n",
       "      <td>1.060779</td>\n",
       "      <td>51.232700</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>-0.119134</td>\n",
       "      <td>-0.118282</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.000852</td>\n",
       "      <td>-1.182823</td>\n",
       "      <td>-1.191344</td>\n",
       "      <td>-1.977614</td>\n",
       "      <td>-1.970009</td>\n",
       "      <td>0.986507</td>\n",
       "      <td>-0.742725</td>\n",
       "      <td>-0.012223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.021800</td>\n",
       "      <td>1.057116</td>\n",
       "      <td>49.579700</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>-0.118487</td>\n",
       "      <td>-0.117852</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.000635</td>\n",
       "      <td>-1.178524</td>\n",
       "      <td>-1.184870</td>\n",
       "      <td>-1.977609</td>\n",
       "      <td>-1.970600</td>\n",
       "      <td>0.982993</td>\n",
       "      <td>-0.741228</td>\n",
       "      <td>-0.009247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.086600</td>\n",
       "      <td>1.053238</td>\n",
       "      <td>49.681400</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>-0.117816</td>\n",
       "      <td>-0.117392</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>-1.173918</td>\n",
       "      <td>-1.178163</td>\n",
       "      <td>-1.977036</td>\n",
       "      <td>-1.970642</td>\n",
       "      <td>0.979258</td>\n",
       "      <td>-0.739795</td>\n",
       "      <td>-0.006354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.035100</td>\n",
       "      <td>1.049138</td>\n",
       "      <td>49.506300</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>-0.117106</td>\n",
       "      <td>-0.116858</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>-1.168582</td>\n",
       "      <td>-1.171058</td>\n",
       "      <td>-1.975693</td>\n",
       "      <td>-1.970134</td>\n",
       "      <td>0.975281</td>\n",
       "      <td>-0.738570</td>\n",
       "      <td>-0.003923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.305100</td>\n",
       "      <td>1.045185</td>\n",
       "      <td>49.009900</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>-0.116388</td>\n",
       "      <td>-0.116332</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-1.163317</td>\n",
       "      <td>-1.163877</td>\n",
       "      <td>-1.973634</td>\n",
       "      <td>-1.968612</td>\n",
       "      <td>0.971461</td>\n",
       "      <td>-0.737240</td>\n",
       "      <td>-0.001268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>1.041333</td>\n",
       "      <td>51.240800</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>-0.115633</td>\n",
       "      <td>-0.115727</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>-1.157275</td>\n",
       "      <td>-1.156327</td>\n",
       "      <td>-1.970495</td>\n",
       "      <td>-1.966067</td>\n",
       "      <td>0.967714</td>\n",
       "      <td>-0.736192</td>\n",
       "      <td>0.000791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.127300</td>\n",
       "      <td>1.037232</td>\n",
       "      <td>48.812800</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>-0.114830</td>\n",
       "      <td>-0.115057</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>-1.150570</td>\n",
       "      <td>-1.148301</td>\n",
       "      <td>-1.965436</td>\n",
       "      <td>-1.961623</td>\n",
       "      <td>0.963701</td>\n",
       "      <td>-0.735307</td>\n",
       "      <td>0.002560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>1.033273</td>\n",
       "      <td>49.355000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>-0.114036</td>\n",
       "      <td>-0.114356</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>-1.143558</td>\n",
       "      <td>-1.140360</td>\n",
       "      <td>-1.960953</td>\n",
       "      <td>-1.957831</td>\n",
       "      <td>0.959808</td>\n",
       "      <td>-0.734645</td>\n",
       "      <td>0.003846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.991200</td>\n",
       "      <td>1.029181</td>\n",
       "      <td>49.104400</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>-0.113182</td>\n",
       "      <td>-0.113569</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>-1.135690</td>\n",
       "      <td>-1.131820</td>\n",
       "      <td>-1.956114</td>\n",
       "      <td>-1.953531</td>\n",
       "      <td>0.955763</td>\n",
       "      <td>-0.734174</td>\n",
       "      <td>0.004807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.927900</td>\n",
       "      <td>1.025012</td>\n",
       "      <td>49.064400</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>-0.112326</td>\n",
       "      <td>-0.112752</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>-1.127520</td>\n",
       "      <td>-1.123256</td>\n",
       "      <td>-1.948832</td>\n",
       "      <td>-1.946618</td>\n",
       "      <td>0.951632</td>\n",
       "      <td>-0.733798</td>\n",
       "      <td>0.005496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.076300</td>\n",
       "      <td>1.021223</td>\n",
       "      <td>49.930000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>-0.111514</td>\n",
       "      <td>-0.111961</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>-1.119606</td>\n",
       "      <td>-1.115142</td>\n",
       "      <td>-1.944614</td>\n",
       "      <td>-1.942826</td>\n",
       "      <td>0.947864</td>\n",
       "      <td>-0.733590</td>\n",
       "      <td>0.005905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.017100</td>\n",
       "      <td>1.017608</td>\n",
       "      <td>50.287000</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>-0.110761</td>\n",
       "      <td>-0.111212</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>-1.112125</td>\n",
       "      <td>-1.107610</td>\n",
       "      <td>-1.939377</td>\n",
       "      <td>-1.937919</td>\n",
       "      <td>0.944267</td>\n",
       "      <td>-0.733410</td>\n",
       "      <td>0.006236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.956800</td>\n",
       "      <td>1.014166</td>\n",
       "      <td>48.675100</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>-0.110061</td>\n",
       "      <td>-0.110515</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>-1.105148</td>\n",
       "      <td>-1.100609</td>\n",
       "      <td>-1.934850</td>\n",
       "      <td>-1.933612</td>\n",
       "      <td>0.940835</td>\n",
       "      <td>-0.733300</td>\n",
       "      <td>0.006575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.066700</td>\n",
       "      <td>1.010652</td>\n",
       "      <td>49.120500</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>-0.109352</td>\n",
       "      <td>-0.109835</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>-1.098350</td>\n",
       "      <td>-1.093521</td>\n",
       "      <td>-1.931926</td>\n",
       "      <td>-1.930270</td>\n",
       "      <td>0.937344</td>\n",
       "      <td>-0.733077</td>\n",
       "      <td>0.007311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.123500</td>\n",
       "      <td>1.006837</td>\n",
       "      <td>48.433400</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>-0.108668</td>\n",
       "      <td>-0.109167</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>-1.091671</td>\n",
       "      <td>-1.086684</td>\n",
       "      <td>-1.929760</td>\n",
       "      <td>-1.926836</td>\n",
       "      <td>0.933537</td>\n",
       "      <td>-0.732992</td>\n",
       "      <td>0.007906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.185700</td>\n",
       "      <td>1.003550</td>\n",
       "      <td>49.214400</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>-0.108042</td>\n",
       "      <td>-0.108576</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>-1.085757</td>\n",
       "      <td>-1.080415</td>\n",
       "      <td>-1.927354</td>\n",
       "      <td>-1.923993</td>\n",
       "      <td>0.930275</td>\n",
       "      <td>-0.732758</td>\n",
       "      <td>0.008908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.136500</td>\n",
       "      <td>0.999687</td>\n",
       "      <td>49.271400</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>-0.107276</td>\n",
       "      <td>-0.107930</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>-1.079304</td>\n",
       "      <td>-1.072763</td>\n",
       "      <td>-1.926422</td>\n",
       "      <td>-1.923238</td>\n",
       "      <td>0.926486</td>\n",
       "      <td>-0.732015</td>\n",
       "      <td>0.010775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.992400</td>\n",
       "      <td>0.995947</td>\n",
       "      <td>49.260500</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>-0.106444</td>\n",
       "      <td>-0.107277</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>-1.072770</td>\n",
       "      <td>-1.064436</td>\n",
       "      <td>-1.925826</td>\n",
       "      <td>-1.923010</td>\n",
       "      <td>0.922866</td>\n",
       "      <td>-0.730810</td>\n",
       "      <td>0.013468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.832300</td>\n",
       "      <td>0.992089</td>\n",
       "      <td>49.267600</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>-0.105625</td>\n",
       "      <td>-0.106563</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>-1.065632</td>\n",
       "      <td>-1.056249</td>\n",
       "      <td>-1.927084</td>\n",
       "      <td>-1.924436</td>\n",
       "      <td>0.919077</td>\n",
       "      <td>-0.730120</td>\n",
       "      <td>0.015379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.855700</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>49.371000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>-0.104878</td>\n",
       "      <td>-0.105820</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>-1.058202</td>\n",
       "      <td>-1.048777</td>\n",
       "      <td>-1.929029</td>\n",
       "      <td>-1.926478</td>\n",
       "      <td>0.915206</td>\n",
       "      <td>-0.729986</td>\n",
       "      <td>0.016152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.068100</td>\n",
       "      <td>0.984501</td>\n",
       "      <td>50.995400</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>-0.104214</td>\n",
       "      <td>-0.105074</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>-1.050741</td>\n",
       "      <td>-1.042141</td>\n",
       "      <td>-1.929647</td>\n",
       "      <td>-1.926986</td>\n",
       "      <td>0.911453</td>\n",
       "      <td>-0.730484</td>\n",
       "      <td>0.015749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.892400</td>\n",
       "      <td>0.980827</td>\n",
       "      <td>49.358600</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>-0.103497</td>\n",
       "      <td>-0.104316</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>-1.043160</td>\n",
       "      <td>-1.034975</td>\n",
       "      <td>-1.933320</td>\n",
       "      <td>-1.930548</td>\n",
       "      <td>0.907723</td>\n",
       "      <td>-0.731035</td>\n",
       "      <td>0.015856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.818400</td>\n",
       "      <td>0.977323</td>\n",
       "      <td>53.173800</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>-0.102774</td>\n",
       "      <td>-0.103562</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>-1.035620</td>\n",
       "      <td>-1.027738</td>\n",
       "      <td>-1.935429</td>\n",
       "      <td>-1.932487</td>\n",
       "      <td>0.904159</td>\n",
       "      <td>-0.731635</td>\n",
       "      <td>0.015978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.970300</td>\n",
       "      <td>0.973854</td>\n",
       "      <td>49.831200</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>-0.102028</td>\n",
       "      <td>-0.102820</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>-1.028198</td>\n",
       "      <td>-1.020279</td>\n",
       "      <td>-1.937445</td>\n",
       "      <td>-1.934514</td>\n",
       "      <td>0.900640</td>\n",
       "      <td>-0.732140</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.010500</td>\n",
       "      <td>0.970490</td>\n",
       "      <td>50.736400</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>-0.101281</td>\n",
       "      <td>-0.102075</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>-1.020747</td>\n",
       "      <td>-1.012807</td>\n",
       "      <td>-1.940878</td>\n",
       "      <td>-1.938147</td>\n",
       "      <td>0.897203</td>\n",
       "      <td>-0.732864</td>\n",
       "      <td>0.016963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.922400</td>\n",
       "      <td>0.967358</td>\n",
       "      <td>49.815800</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>-0.100525</td>\n",
       "      <td>-0.101366</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>-1.013660</td>\n",
       "      <td>-1.005247</td>\n",
       "      <td>-1.944620</td>\n",
       "      <td>-1.941984</td>\n",
       "      <td>0.894014</td>\n",
       "      <td>-0.733437</td>\n",
       "      <td>0.017940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.055700</td>\n",
       "      <td>0.964099</td>\n",
       "      <td>52.899700</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>-0.099782</td>\n",
       "      <td>-0.100665</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>-1.006647</td>\n",
       "      <td>-0.997816</td>\n",
       "      <td>-1.947582</td>\n",
       "      <td>-1.944990</td>\n",
       "      <td>0.890679</td>\n",
       "      <td>-0.734195</td>\n",
       "      <td>0.018726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.104400</td>\n",
       "      <td>0.961138</td>\n",
       "      <td>48.910400</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>-0.099080</td>\n",
       "      <td>-0.100028</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>-1.000278</td>\n",
       "      <td>-0.990802</td>\n",
       "      <td>-1.950705</td>\n",
       "      <td>-1.948157</td>\n",
       "      <td>0.887645</td>\n",
       "      <td>-0.734931</td>\n",
       "      <td>0.019854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.029600</td>\n",
       "      <td>0.958371</td>\n",
       "      <td>50.395500</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.397000</td>\n",
       "      <td>-0.098482</td>\n",
       "      <td>-0.099476</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>-0.994758</td>\n",
       "      <td>-0.984817</td>\n",
       "      <td>-1.953329</td>\n",
       "      <td>-1.950899</td>\n",
       "      <td>0.884804</td>\n",
       "      <td>-0.735663</td>\n",
       "      <td>0.020642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955960</td>\n",
       "      <td>49.421600</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>-0.097948</td>\n",
       "      <td>-0.098990</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>-0.989901</td>\n",
       "      <td>-0.979482</td>\n",
       "      <td>-1.956008</td>\n",
       "      <td>-1.953578</td>\n",
       "      <td>0.882347</td>\n",
       "      <td>-0.736126</td>\n",
       "      <td>0.021738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>0.953712</td>\n",
       "      <td>49.424500</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>-0.097462</td>\n",
       "      <td>-0.098572</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>-0.985716</td>\n",
       "      <td>-0.974619</td>\n",
       "      <td>-1.959013</td>\n",
       "      <td>-1.956609</td>\n",
       "      <td>0.880088</td>\n",
       "      <td>-0.736240</td>\n",
       "      <td>0.023401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.951761</td>\n",
       "      <td>50.925400</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>-0.097076</td>\n",
       "      <td>-0.098234</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>-0.982341</td>\n",
       "      <td>-0.970762</td>\n",
       "      <td>-1.962378</td>\n",
       "      <td>-1.959936</td>\n",
       "      <td>0.878125</td>\n",
       "      <td>-0.736355</td>\n",
       "      <td>0.024739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.027000</td>\n",
       "      <td>0.949880</td>\n",
       "      <td>51.375700</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>-0.096735</td>\n",
       "      <td>-0.097945</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>-0.979454</td>\n",
       "      <td>-0.967347</td>\n",
       "      <td>-1.963813</td>\n",
       "      <td>-1.961411</td>\n",
       "      <td>0.876256</td>\n",
       "      <td>-0.736245</td>\n",
       "      <td>0.026173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.907800</td>\n",
       "      <td>0.948289</td>\n",
       "      <td>51.857100</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.386000</td>\n",
       "      <td>-0.096456</td>\n",
       "      <td>-0.097689</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>-0.976892</td>\n",
       "      <td>-0.964558</td>\n",
       "      <td>-1.967137</td>\n",
       "      <td>-1.964771</td>\n",
       "      <td>0.874668</td>\n",
       "      <td>-0.736206</td>\n",
       "      <td>0.027157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.796900</td>\n",
       "      <td>0.946729</td>\n",
       "      <td>50.970800</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>-0.096227</td>\n",
       "      <td>-0.097479</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>-0.974793</td>\n",
       "      <td>-0.962271</td>\n",
       "      <td>-1.969438</td>\n",
       "      <td>-1.966926</td>\n",
       "      <td>0.873112</td>\n",
       "      <td>-0.736178</td>\n",
       "      <td>0.027767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/20 00:10 < 00:29, 0.47 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(new_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:2278\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:2662\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2660\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2662\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2663\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:3467\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3464\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3466\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3467\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3477\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:902\u001b[0m, in \u001b[0;36mORPOTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# Base evaluation\u001b[39;00m\n\u001b[0;32m--> 902\u001b[0m initial_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m initial_output\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:3650\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3647\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3649\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3650\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3651\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3652\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:837\u001b[0m, in \u001b[0;36mORPOTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m    834\u001b[0m prediction_context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_has_been_casted_to_bf16 \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), prediction_context_manager():\n\u001b[0;32m--> 837\u001b[0m     loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_loss_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# force log the metrics\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_metrics(metrics, train_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:746\u001b[0m, in \u001b[0;36mORPOTrainer.get_batch_loss_metrics\u001b[0;34m(self, model, batch, train_eval)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the ORPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\u001b[39;00m\n\u001b[1;32m    738\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    740\u001b[0m (\n\u001b[1;32m    741\u001b[0m     policy_chosen_logps,\n\u001b[1;32m    742\u001b[0m     policy_rejected_logps,\n\u001b[1;32m    743\u001b[0m     policy_chosen_logits,\n\u001b[1;32m    744\u001b[0m     policy_rejected_logits,\n\u001b[1;32m    745\u001b[0m     policy_nll_loss,\n\u001b[0;32m--> 746\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenated_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m losses, chosen_rewards, rejected_rewards, log_odds_ratio, log_odds_chosen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39modds_ratio_loss(\n\u001b[1;32m    749\u001b[0m     policy_chosen_logps, policy_rejected_logps\n\u001b[1;32m    750\u001b[0m )\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# full ORPO loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:686\u001b[0m, in \u001b[0;36mORPOTrainer.concatenated_forward\u001b[0;34m(self, model, batch)\u001b[0m\n\u001b[1;32m    676\u001b[0m len_chosen \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchosen_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    678\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    679\u001b[0m     {\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shift_right(concatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    684\u001b[0m )\n\u001b[0;32m--> 686\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcatenated_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcatenated_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcatenated_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcatenated_attention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m all_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy_loss\u001b[39m(logits, labels):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/peft/peft_model.py:1129\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1128\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1208\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1205\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1221\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1018\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1008\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1009\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         cache_position,\n\u001b[1;32m   1016\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1018\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:741\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:490\u001b[0m, in \u001b[0;36mLlamaFlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mto(target_dtype)\n\u001b[1;32m    488\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mto(target_dtype)\n\u001b[0;32m--> 490\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flash_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    495\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:533\u001b[0m, in \u001b[0;36mLlamaFlashAttention2._flash_attention_forward\u001b[0;34m(self, query_states, key_states, value_states, attention_mask, query_length, dropout, softmax_scale)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 533\u001b[0m     query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upad_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_length\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     cu_seqlens_q, cu_seqlens_k \u001b[38;5;241m=\u001b[39m cu_seq_lens\n\u001b[1;32m    538\u001b[0m     max_seqlen_in_batch_q, max_seqlen_in_batch_k \u001b[38;5;241m=\u001b[39m max_seq_lens\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:562\u001b[0m, in \u001b[0;36mLlamaFlashAttention2._upad_input\u001b[0;34m(self, query_layer, key_layer, value_layer, attention_mask, query_length)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_upad_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_layer, key_layer, value_layer, attention_mask, query_length):\n\u001b[0;32m--> 562\u001b[0m     indices_k, cu_seqlens_k, max_seqlen_in_batch_k \u001b[38;5;241m=\u001b[39m \u001b[43m_get_unpad_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m     batch_size, kv_seq_len, num_key_value_heads, head_dim \u001b[38;5;241m=\u001b[39m key_layer\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    565\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m index_first_axis(\n\u001b[1;32m    566\u001b[0m         key_layer\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m kv_seq_len, num_key_value_heads, head_dim), indices_k\n\u001b[1;32m    567\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:66\u001b[0m, in \u001b[0;36m_get_unpad_data\u001b[0;34m(attention_mask)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_unpad_data\u001b[39m(attention_mask):\n\u001b[1;32m     65\u001b[0m     seqlens_in_batch \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m---> 66\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     67\u001b[0m     max_seqlen_in_batch \u001b[38;5;241m=\u001b[39m seqlens_in_batch\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     68\u001b[0m     cu_seqlens \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(torch\u001b[38;5;241m.\u001b[39mcumsum(seqlens_in_batch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cd766-26d0-4f78-83e6-9299a15f3380",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72929c2-1bae-46c1-8571-02ca5a40e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush memory\n",
    "del trainer, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "# Merge adapter with base model\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c114762-3b53-46d8-b1b1-90e72bc929ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a71cb6-3026-408b-8c33-452308193295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
