{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6edf12-5153-462c-8953-3a9c143b025b",
   "metadata": {},
   "source": [
    "# Source: https://github.com/apple/corenet/blob/main/tutorials/train_a_new_model_on_a_new_dataset_from_scratch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b61756-4915-46f0-b502-647318f6621d",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d3c76a-7b86-45e1-9b41-66ddd09a0adf",
   "metadata": {},
   "source": [
    "## Repository: https://github.com/apple/corenet/tree/main"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17fad52b-727f-4acd-aed4-c9f14f639107",
   "metadata": {},
   "source": [
    "sudo apt install git-lfs\n",
    "\n",
    "git clone git@github.com:apple/corenet.git\n",
    "cd corenet\n",
    "git lfs install\n",
    "git lfs pull\n",
    "# The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.\n",
    "python3 -m venv venv && source venv/bin/activate\n",
    "python3 -m pip install --editable ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0337559-4df4-4a30-a204-d6c9b3560dae",
   "metadata": {},
   "source": [
    "# Create structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44f3465-1f83-4784-9d07-8537614ac9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p corenet/projects/playground_cifar10/classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c990186-7e58-4691-b751-6218bd3382c0",
   "metadata": {},
   "source": [
    "# Create config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404fc7d5-4cff-46f5-a94c-a75d9f549b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing corenet/projects/playground_cifar10/classification/cifar10.yaml\n"
     ]
    }
   ],
   "source": [
    "%%file corenet/projects/playground_cifar10/classification/cifar10.yaml\n",
    "\n",
    "common:\n",
    "    log_freq: 2000                 # Log the training metrics every 2000 iterations.\n",
    "\n",
    "dataset:\n",
    "    category: classification\n",
    "    name: \"cifar10\"                # We'll register the \"cifar10\" name at DATASET_REGISTRY later in this tutorial.\n",
    "\n",
    "    # The `corenet-train` entrypoint uses train_batch_size0 and val_batch_size0 values to construct \n",
    "    # training/validation batches during training. The `corenet-eval` entrypoint uses eval_batch_size0 to \n",
    "    # construct batches during evaluation (ie test).\n",
    "    #\n",
    "    # The effective batch size is: num_nodes x num_gpus x train_batch_size0\n",
    "    train_batch_size0: 4\n",
    "    val_batch_size0: 4\n",
    "    eval_batch_size0: 1\n",
    "\n",
    "    workers: 2\n",
    "    persistent_workers: true\n",
    "    pin_memory: true\n",
    "\n",
    "model:\n",
    "    classification:\n",
    "        name: \"two_layer\"          # We'll register the \"two_layer\" name at MODEL_REGISTRY later in this tutorial.\n",
    "        n_classes: 10\n",
    "\n",
    "    layer:\n",
    "        # Weight initialization parameters:\n",
    "        conv_init: \"kaiming_normal\"\n",
    "        linear_init: \"trunc_normal\"\n",
    "        linear_init_std_dev: 0.02\n",
    "\n",
    "\n",
    "sampler:\n",
    "    name: batch_sampler\n",
    "\n",
    "    # The following dimensions will be passed to the dataset.__get__ method, and the dataset produces samples \n",
    "    # cropped and resized to the requested dimensions. \n",
    "    bs:\n",
    "        crop_size_width: 32\n",
    "        crop_size_height: 32\n",
    "\n",
    "loss:\n",
    "    category: classification\n",
    "    classification:\n",
    "        name: cross_entropy       # The implemention is available in \"corenet/loss_fn/\" folder.\n",
    "\n",
    "optim:\n",
    "    name: sgd\n",
    "    sgd:\n",
    "        momentum: 0.9\n",
    "\n",
    "scheduler:\n",
    "    name: fixed                    # The implementation is available in \"corenet/optims/scheduler/\" folder.\n",
    "    max_epochs: 2\n",
    "    fixed:\n",
    "        lr: 0.001                  # Fixed Learning Rate\n",
    "\n",
    "stats:\n",
    "  val: [\"loss\", \"top1\"]            # Metrics to log\n",
    "  train: [\"loss\", \"top1\"]\n",
    "  checkpoint_metric: top1          # Assigns a checkpoint to results/checkpoint_best.pt\n",
    "  checkpoint_metric_max: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b9772d-6188-4965-9229-88849c72d925",
   "metadata": {},
   "source": [
    "# Register Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec4c6c0-abb9-4cb2-bb52-4300cda593f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing corenet/corenet/data/datasets/classification/playground_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%file corenet/corenet/data/datasets/classification/playground_dataset.py\n",
    "\n",
    "from argparse import Namespace\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from corenet.data.datasets import DATASET_REGISTRY\n",
    "from corenet.data.datasets.dataset_base import BaseDataset\n",
    "\n",
    "\n",
    "@DATASET_REGISTRY.register(name=\"cifar10\", type=\"classification\")\n",
    "class Cifar10(BaseDataset):\n",
    "    CLASS_NAMES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    def __init__(self, opts: Namespace, **kwargs) -> None:\n",
    "        super().__init__(opts, **kwargs)\n",
    "        self._torchvision_dataset = torchvision.datasets.CIFAR10(\n",
    "            \"/tmp/cifar10_cache\",\n",
    "            train=self.is_training,\n",
    "            download=True,\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._torchvision_dataset)\n",
    "\n",
    "    def __getitem__(self, sample_size_and_index: Tuple[int]) -> Dict[str, Any]:\n",
    "        # In CoreNet, not only does the sampler determine the index of the samples, but\n",
    "        # also the sampler determines the crop size dynamically for each batch. This\n",
    "        # allows samplers to train multi-scale models more efficiently.\n",
    "        # See: corenet/data/sampler/variable_batch_sampler.py\n",
    "        (crop_size_h, crop_size_w, sample_index) = sample_size_and_index\n",
    "\n",
    "        img, target = self._torchvision_dataset[sample_index]\n",
    "\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                transforms.Resize(size=(crop_size_h, crop_size_w)),\n",
    "            ]\n",
    "        )\n",
    "        img = transform(img)\n",
    "        return {\n",
    "            \"samples\": img,\n",
    "            \"targets\": target,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0dd17b-d756-43d1-88bb-b6f6775f6647",
   "metadata": {},
   "source": [
    "# Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d2d32d4-06f8-46ef-b1da-2427316aee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing corenet/corenet/modeling/models/classification/playground_model.py\n"
     ]
    }
   ],
   "source": [
    "%%file corenet/corenet/modeling/models/classification/playground_model.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from corenet.modeling.models import MODEL_REGISTRY\n",
    "from corenet.modeling.models.base_model import BaseAnyNNModel\n",
    "\n",
    "\n",
    "@MODEL_REGISTRY.register(\"two_layer\", type=\"classification\")\n",
    "class Net(BaseAnyNNModel):\n",
    "    \"\"\"A simple 2-layer CNN, inspired by https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\"\"\"\n",
    "\n",
    "    def __init__(self, opts: argparse.Namespace) -> None:\n",
    "        super().__init__(opts)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.reset_parameters(opts)  # Initialize the weights\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524f345-f977-4b2d-9097-9705bc076f3f",
   "metadata": {},
   "source": [
    "# Start training (in shell in corenet directory)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86b029b7-ab16-4a0f-8b4e-08e5ca0c7e39",
   "metadata": {},
   "source": [
    "corenet-train --common.config-file projects/playground_cifar10/classification/cifar10.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5014b55-cec8-4314-a8b3-b2881e7e4697",
   "metadata": {},
   "source": [
    "# Evaulate after training (in shell in corenet directory)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62b8f07a-7a2c-446f-851e-d2c1cb39b920",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0 corenet-eval \\\n",
    "    --common.config-file projects/playground_cifar10/classification/cifar10.yaml \\\n",
    "    --model.classification.pretrained results/run_1/checkpoint_best.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541a06c-48cf-4c7e-a396-47a8376bf4e2",
   "metadata": {},
   "source": [
    "# Visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28470a6a-23b2-4804-b0f9-8cf273e0dd9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'corenet.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcorenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_training_arguments\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcorenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[0;32m~/repos/ai-playground/notebooks/reproduce/apple_corent/corenet/corenet/options/opts.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcorenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollate_fns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arguments_collate_fn\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcorenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arguments_dataset\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcorenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransfer_clients\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transfer_client_arguments\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'corenet.data'"
     ]
    }
   ],
   "source": [
    "from corenet.options.opts import get_training_arguments\n",
    "from corenet.modeling import get_model\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, PILToTensor, CenterCrop\n",
    "from torchvision.transforms import ToPILImage\n",
    "from corenet.data.datasets.classification.playground_dataset import Cifar10\n",
    "\n",
    "config_file = \"corenet/projects/playground_cifar10/classification/cifar10.yaml\"\n",
    "pretrained_weights = \"corenet/results/run_1/checkpoint_best.pt\"\n",
    "\n",
    "opts = get_training_arguments(\n",
    "    args=[\n",
    "        \"--common.config-file\",\n",
    "        config_file,\n",
    "        \"--model.classification.pretrained\",\n",
    "        pretrained_weights,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = get_model(opts)\n",
    "model.eval()\n",
    "\n",
    "for image_path in [\"corenet/assets/cat.jpeg\", \"corenet/assets/dog.jpeg\"]:\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_transforms = Compose([CenterCrop(600), Resize(size=(32, 32)), PILToTensor()])\n",
    "\n",
    "    # Transform the image, normalize between 0 and 1\n",
    "    input_tensor = img_transforms(image)\n",
    "\n",
    "    # Show the transformed image\n",
    "    ToPILImage()(input_tensor).show()\n",
    "\n",
    "    input_tensor = input_tensor.to(torch.float).div(255.0)\n",
    "\n",
    "    # add dummy batch dimension\n",
    "    input_tensor = input_tensor[None, ...]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[0]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        predictions = sorted(zip(probs.tolist(), Cifar10.CLASS_NAMES), reverse=True)\n",
    "        print(\n",
    "            \"Top 3 Predictions:\",\n",
    "            [f\"{cls}: {prob:.1%}\" for prob, cls in predictions[:3]],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925bb8a-9188-40c1-8a71-efea72acec40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
