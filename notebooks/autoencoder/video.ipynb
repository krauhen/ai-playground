{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89718511-1f6a-4147-94bd-62e83d94198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torch_data\n",
    "import torchvision as tv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lpips import LPIPS\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f956a61b-84d6-4be7-9553-0efc9cf020a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch_data.Dataset):\n",
    "    def __init__(self, img_size, seq_len, train=True, transform=None):\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.elements = []\n",
    "        self.elements.extend(glob.glob(\"/mnt/data/youtube/latent/*\"))\n",
    "        self.elements = sorted(self.elements)\n",
    "\n",
    "        n_train = int(len(self.elements) * 0.8)\n",
    "        if train:\n",
    "            self.elements = self.elements[:n_train]\n",
    "        else:\n",
    "            self.elements = self.elements[n_train:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.elements)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        BASE_PATH_IMG = \"/mnt/data/youtube/img_latent\"\n",
    "        BASE_PATH_LATENT = \"/mnt/data/youtube/latent\"\n",
    "\n",
    "        filename = self.elements[idx]\n",
    "        filename = filename.split(\"/\")[-1]\n",
    "        filename_base = filename.split(\".\")[0]\n",
    "        filename_parts = filename_base.split(\"_\")\n",
    "        filename = \"_\".join(filename_parts[:-1])\n",
    "        frame_idx = filename_parts[-1]\n",
    "\n",
    "        frame_indices = []\n",
    "        filenames = glob.glob(os.path.join(BASE_PATH_LATENT, f\"{filename}*\"))\n",
    "        for filename in filenames:\n",
    "            filename = filename.split(\"/\")[-1]\n",
    "            filename_base = filename.split(\".\")[0]\n",
    "            filename_parts = filename_base.split(\"_\")\n",
    "            filename = \"_\".join(filename_parts[:-1])\n",
    "            frame_idx = filename_parts[-1]\n",
    "            frame_idx = frame_idx.split(\".\")[0]\n",
    "            frame_indices.append(frame_idx)\n",
    "        frame_indices = np.sort(np.asarray(frame_indices, dtype=int))\n",
    "\n",
    "        idx = np.random.randint(0, len(frame_indices) - self.seq_len - 1)\n",
    "        latent_imgs = np.zeros([self.seq_len, 64, 64, 4], dtype=np.float32)\n",
    "        for i in range(self.seq_len):\n",
    "            frame_idx = frame_indices[idx + i]\n",
    "            idx_filename = os.path.join(BASE_PATH_LATENT, f\"{filename}_{frame_idx}.png\")\n",
    "            with open(idx_filename, \"rb\") as f:\n",
    "                latent = np.load(f)\n",
    "            latent_imgs[i] = latent\n",
    "\n",
    "        last_idx = idx + self.seq_len\n",
    "        frame_idx = frame_indices[last_idx]\n",
    "\n",
    "        idx_filename = os.path.join(BASE_PATH_LATENT, f\"{filename}_{frame_idx}.png\")\n",
    "        with open(idx_filename, \"rb\") as f:\n",
    "            latent = np.load(f)\n",
    "        \n",
    "        filename = os.path.join(BASE_PATH_IMG, f\"{filename}_{frame_idx}.png\")\n",
    "        img = cv2.imread(filename)\n",
    "\n",
    "        img[:, :, [0, 1, 2]] = img[:, :, [2, 1, 0]]\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        img = (np.asarray(img) / 128) - 1\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        latent_imgs = latent_imgs.transpose(0, 3, 1, 2)\n",
    "        latent_imgs = np.asarray(latent_imgs)\n",
    "        latent_imgs = torch.from_numpy(latent_imgs)\n",
    "\n",
    "        latent = latent.transpose(2, 0, 1)\n",
    "        latent = np.asarray(latent)\n",
    "        latent = torch.from_numpy(latent)\n",
    "\n",
    "        element = dict()\n",
    "        element[\"img\"] = img\n",
    "        element[\"latent_seq\"] = latent_imgs\n",
    "        element[\"latent\"] = latent\n",
    "        \n",
    "        if self.transform:\n",
    "            element = self.transform(element)\n",
    "\n",
    "        return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f6720abd-b23f-4237-9fa2-a42f690172dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "BATCH_SUM = 16\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "N_HEADS = 12\n",
    "N_LAYER = 12\n",
    "N_EMB = 512\n",
    "BLOCK_SIZE = 64\n",
    "DROPOUT = 0.1\n",
    "IMG_SIZE = 512\n",
    "F_IN = 4 * 64 * 64\n",
    "SEQ_LEN = 4\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "N_EPOCHS = 100\n",
    "FILE_PATH = f\"weights/video.pth\"\n",
    "FILE_NAME_DECODER = f\"weights/decoder_{IMG_SIZE}.pth\"\n",
    "FILE_NAME_REFINER = f\"weights/refiner_{IMG_SIZE}.pth\"\n",
    "GRAD_CLIP = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a62f12e0-acfc-47b1-a8cd-f6bdf6ecaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(IMG_SIZE, SEQ_LEN, train=True)\n",
    "test_dataset = Dataset(IMG_SIZE, SEQ_LEN, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "05dfb8ca-1904-426f-b6d2-02941c3ce047",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch_data.DataLoader(train_dataset, shuffle=True, pin_memory=True, num_workers=4, batch_size=BATCH_SIZE)\n",
    "test_loader = torch_data.DataLoader(test_dataset, shuffle=True, pin_memory=True, num_workers=4, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7a4aaa5a-5977-4016-b2d5-2212ca7de9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8892\n",
      "2224\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a16bac1c-03f4-4ca1-8a17-74db21f04580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "\n",
    "    def forward(self, x, causal_mask=False):\n",
    "        input_shape = x.shape\n",
    "        batch_size, sequence_length, d_embed = input_shape\n",
    "        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
    "\n",
    "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "        q = q.view(interim_shape).transpose(1, 2)\n",
    "        k = k.view(interim_shape).transpose(1, 2)\n",
    "        v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "        weight = q @ k.transpose(-1, -2)\n",
    "        if causal_mask:\n",
    "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
    "            weight.masked_fill_(mask, -torch.inf)\n",
    "        weight /= math.sqrt(self.d_head)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        output = weight @ v\n",
    "        output = output.transpose(1, 2)\n",
    "        output = output.reshape(input_shape)\n",
    "        output = self.out_proj(output)\n",
    "        return output\n",
    "class AttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(32, channels)\n",
    "        self.attention = SelfAttention(1, channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residue = x\n",
    "        x = self.groupnorm(x)\n",
    "\n",
    "        n, c, h, w = x.shape\n",
    "        x = x.view((n, c, h * w))\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = self.attention(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = x.view((n, c, h, w))\n",
    "\n",
    "        x += residue\n",
    "        return x\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residue = x\n",
    "\n",
    "        x = self.groupnorm_1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = self.groupnorm_2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x + self.residual_layer(residue)\n",
    "class Decoder(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(4, 4, kernel_size=1, padding=0),\n",
    "            nn.Conv2d(4, 512, kernel_size=3, padding=1),\n",
    "            ResidualBlock(512, 512),\n",
    "            AttentionBlock(512),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            ResidualBlock(512, 256),\n",
    "            ResidualBlock(256, 256),\n",
    "            ResidualBlock(256, 256),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            ResidualBlock(256, 128),\n",
    "            ResidualBlock(128, 128),\n",
    "            ResidualBlock(128, 128),\n",
    "            nn.GroupNorm(32, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(128, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x /= 0.18215\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "        return x\n",
    "class Refiner(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(3, 128, kernel_size=2, padding=1),\n",
    "            ResidualBlock(128, 128),\n",
    "            ResidualBlock(128, 128),\n",
    "            nn.GroupNorm(32, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(128, 3, kernel_size=2, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1a8b8b5e-f231-48ae-8331-d9f3a64dcce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "\n",
    "        if mask is not None:\n",
    "            wei = wei.masked_fill(mask == 0, float('-inf')) # (B, T, T)\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.sa(self.ln1(x), mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, f_in, n_embd, block_size, n_head, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.embedding = nn.Linear(f_in, 512)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, f_in)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "\n",
    "        B, SEQ, C, H, W = idx.shape\n",
    "        idx = idx.view(B, SEQ, -1)\n",
    "        \n",
    "        device = idx.device\n",
    "        mask = None\n",
    "\n",
    "        emb = self.embedding(idx)\n",
    "        x = self.dropout(emb)\n",
    "        for i in range(len(self.blocks)):\n",
    "            x = self.blocks[i](x, mask)\n",
    "        x = x[:, -1]\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        logits = logits.view(B, C, H, W)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=1):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            next_logits = logits[:, -1, :]\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3de5a4c1-93ad-4c56-935a-ee74fd308bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(pred, img):\n",
    "    loss_mse = nn.MSELoss()(pred, img)\n",
    "    \n",
    "    loss = 0\n",
    "    loss += loss_mse\n",
    "\n",
    "    return loss\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    iterator = iter(train_loader)\n",
    "    N = int(np.floor(len(train_loader) / BATCH_SUM))\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        for j in range(BATCH_SUM):\n",
    "            batch = next(iterator)\n",
    "            \n",
    "            latent_seq = batch[\"latent_seq\"].to(DEVICE)\n",
    "            latent = batch[\"latent\"].to(DEVICE)\n",
    "            img = batch[\"img\"].to(DEVICE)\n",
    "            \n",
    "            pred = model(latent_seq)\n",
    "            \n",
    "            loss = criterion(pred, latent)\n",
    "            loss = loss / BATCH_SUM\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            sum_loss += loss.item()\n",
    "            count += 1\n",
    "            \n",
    "            print(f\"\\r{i + 1:06}/{N:06} | {j + 1:03}/{BATCH_SUM:03} loss: {(sum_loss / count) * BATCH_SUM}\", end=\"\")\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if ((i + 1) % 10) == 0:\n",
    "            torch.save(model.state_dict(), FILE_PATH)\n",
    "            sum_loss = 0\n",
    "            count = 0\n",
    "            print()\n",
    "\n",
    "        if ((i + 1) % 10) == 0:\n",
    "            show(N=1, M=1)\n",
    "    \n",
    "    print()\n",
    "@torch.no_grad()\n",
    "def show(N=1, M=1):\n",
    "    model.eval()\n",
    "\n",
    "    batch = next(iter(test_loader))\n",
    "    \n",
    "    latent_seq = batch[\"latent_seq\"].to(DEVICE)\n",
    "    latent = batch[\"latent\"].to(DEVICE)\n",
    "    img = batch[\"img\"].to(DEVICE)\n",
    "    \n",
    "    pred = model(latent_seq)\n",
    "\n",
    "    BS, SEQ, D, H, W = latent_seq.shape\n",
    "    latent = F.interpolate(latent, (IMG_SIZE, IMG_SIZE), mode=\"bilinear\")\n",
    "    pred = F.interpolate(pred, (IMG_SIZE, IMG_SIZE), mode=\"bilinear\")\n",
    "    latent_seq = F.interpolate(latent_seq.view(BS * SEQ, D, H, W), (IMG_SIZE, IMG_SIZE), mode=\"bilinear\").view(BS, SEQ, D, IMG_SIZE, IMG_SIZE)\n",
    "    \n",
    "    latent_seq = latent_seq.detach().cpu().numpy()\n",
    "    latent = latent.detach().cpu().numpy()\n",
    "    img = img.detach().cpu().numpy()\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "\n",
    "    H = 20\n",
    "    W = int((H / 2.5) * (N / M))\n",
    "    fig, axes = plt.subplots(N, M, figsize=(H, W))\n",
    "\n",
    "    if N == 1 and M == 1:\n",
    "        axes = [axes]\n",
    "    elif N == 1:\n",
    "        axes = [axes]\n",
    "    if M == 1:\n",
    "        axes = [axes]               \n",
    "                             \n",
    "    for n in range(N):\n",
    "        for m in range(M):\n",
    "            idx = n * M + m\n",
    "\n",
    "            latent_seq_ = []\n",
    "            for element in latent_seq[idx]:\n",
    "                element = element - element.min()\n",
    "                element = element / element.max()\n",
    "                element = element[:3]\n",
    "                latent_seq_.append(element)\n",
    "\n",
    "            latent_ = latent[idx]\n",
    "            latent_ = latent_ - latent_.min()\n",
    "            latent_ = latent_ / latent_.max()\n",
    "            latent_ = latent_[:3]\n",
    "            \n",
    "            pred_ = pred[idx]\n",
    "            pred_ = pred_ - pred_.min()\n",
    "            pred_ = pred_ / pred_.max()\n",
    "            pred_ = pred_[:3]\n",
    "\n",
    "            img_ = img[idx]\n",
    "            img_ = (img_ + 1) / 2\n",
    "\n",
    "            filler = latent_.copy()\n",
    "            filler[:] = 0\n",
    "            \n",
    "            img_1 = np.concatenate([img_, pred_, latent_, filler], axis=-1).transpose(1, 2, 0)\n",
    "            img_2 = np.concatenate(latent_seq_, axis=-1).transpose(1, 2, 0)\n",
    "            img_ = np.concatenate([img_1, img_2], axis=0)\n",
    "            \n",
    "            axes[n][m].imshow(img_)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7abb5198-9a94-43c7-80ed-4f5050e81a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78c7290651c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/henning/tmp/experiments/ai/stable_diffusion/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/henning/tmp/experiments/ai/stable_diffusion/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 947, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF_IN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EMB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_HEADS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_LAYER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDROPOUT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(torch.load(FILE_PATH))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;241m.\u001b[39mrequires_grad)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/tmp/experiments/ai/stable_diffusion/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tmp/experiments/ai/stable_diffusion/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/tmp/experiments/ai/stable_diffusion/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/tmp/experiments/ai/stable_diffusion/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/tmp/experiments/ai/stable_diffusion/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:849\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 849\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/tmp/experiments/ai/stable_diffusion/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model(F_IN, N_EMB, BLOCK_SIZE, N_HEADS, N_LAYER, DROPOUT).to(DEVICE)\n",
    "# model.load_state_dict(torch.load(FILE_PATH))\n",
    "print(f\"model: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\".replace(\",\", \".\"))\n",
    "print()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "latent_seq = batch[\"latent_seq\"].to(DEVICE)\n",
    "latent = batch[\"latent\"].to(DEVICE)\n",
    "img = batch[\"img\"].to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(latent_seq)\n",
    "\n",
    "loss = criterion(pred, latent)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1edd27-9051-4a26-86d2-4b45a7114f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(N=1, M=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466552a8-df45-46e8-9ad6-9315ee28552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        n_epoch = 0\n",
    "        for n_epoch in range(n_epoch, N_EPOCHS):\n",
    "            print(f\"{n_epoch + 1}|{N_EPOCHS}\")\n",
    "            show(N=1, M=1)\n",
    "            train()\n",
    "            torch.save(model.state_dict(), FILE_PATH)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        n_epoch = 0\n",
    "        torch.save(model.state_dict(), FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1cbe6-961c-4cc8-85a2-231f893c0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), FILE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
