{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79114862-a791-413e-bfa8-0e804b81a216",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import torch\n",
    "import requests\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "import imageio.v2 as io\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47a4e4-ddca-4c33-8a15-41ed5fd4f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(images, N=4):\n",
    "    x = images.to(device)\n",
    "    y = model(x.to(device))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, N, figsize=(20, 10))\n",
    "    for i in range(min(len(x), N)):\n",
    "        \n",
    "        img = x.detach().cpu().numpy()[i].transpose(1, 2, 0)\n",
    "        axes[0, i].imshow(img, cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(f\"input\")\n",
    "\n",
    "        img = y.detach().cpu().numpy()[i].transpose(1, 2, 0)\n",
    "        img = img - np.min(img)\n",
    "        img = img / np.max(img)\n",
    "        axes[1, i].imshow(img, cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(f\"pred\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe39e6-3574-4bed-9d4f-54bdbf68f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize:\n",
    "    def __init__(self, H, W):\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        c, w, h = img.shape\n",
    "        min_size = min(w, h)\n",
    "        img = img[None, :, :min_size, :min_size]\n",
    "        img = F.interpolate(img, (self.H, self.W), mode=\"bilinear\")\n",
    "        img = img[0]\n",
    "        \n",
    "        if c == 1:\n",
    "            img = img.repeat(3, 1, 1)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be24967-7fa0-49ca-a0ff-927377031f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Laion400MDataset(Dataset):\n",
    "    def __init__(self, root_dir, img_size=256, transform=None, split=0.8, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.train = train\n",
    "\n",
    "        self.img_files = np.sort(glob.glob(os.path.join(root_dir, \"imgs\", \"*\")))\n",
    "\n",
    "        filtered_files = list()\n",
    "        for path in self.img_files:\n",
    "            try:\n",
    "                img = Image.open(path)\n",
    "\n",
    "                if img.mode == \"P\":\n",
    "                    continue\n",
    "                \n",
    "                filtered_files.append(path)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        self.img_files = filtered_files \n",
    "            \n",
    "        if train:\n",
    "            n = int(len(self.img_files) * 0.8)\n",
    "            self.img_files = self.img_files[:n]\n",
    "        else:\n",
    "            n = int(len(self.img_files) * 0.8)\n",
    "            self.img_files = self.img_files[n:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.img_files[idx]\n",
    "        \n",
    "        with Image.open(img_path) as img:\n",
    "\n",
    "            mode = img.mode\n",
    "            img = img.convert('RGB')\n",
    "            img = np.asarray(img)\n",
    "            h, w, c = img.shape\n",
    "            \n",
    "            if h > w:\n",
    "                img_ = np.zeros((h, h, c))\n",
    "                diff = h - w\n",
    "                d1 = diff // 2\n",
    "                d2 = diff - d1\n",
    "                img_[:, d1:d1 + w] = img\n",
    "                img = img_\n",
    "            elif w > h:\n",
    "                img_ = np.zeros((w, w, c))\n",
    "                diff = w - h\n",
    "                d1 = diff // 2\n",
    "                d2 = diff - d1\n",
    "                img_[d1:d1 + h] = img\n",
    "                img = img_\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        img = cv2.resize(img.copy(), (self.img_size, self.img_size))\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        img = np.asarray(img, dtype=np.float32)\n",
    "        \n",
    "        img = img - img.min()\n",
    "        if np.max(img) != 0:\n",
    "            img = img / np.max(img)\n",
    "\n",
    "        element = dict()\n",
    "        element[\"img\"] = img\n",
    "                \n",
    "        return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240244a-222f-454c-9787-4475fa3ee67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\"\n",
    "epoch = 0\n",
    "batch_size = 4\n",
    "N_BS = 32\n",
    "IMG_SIZE = 2048\n",
    "file_path = \"data/autoencoder.pth\"\n",
    "ROOT_PATH = \"/mnt/data/laion400M/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c499ce-6b57-47c2-bd58-df85d9baa30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6675c1-45fd-4a64-a013-18441ff9fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), Resize(IMG_SIZE, IMG_SIZE)])\n",
    "dataset = Laion400MDataset(root_dir=ROOT_PATH, img_size=IMG_SIZE)\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e954356-868f-481f-8051-30bc826b9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bfa1df-e4b6-48b1-87bc-91a0c586e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ch_in=3, ch=32, ch_out=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(ch_in, ch, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(ch, ch, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(ch, ch, 3, 1)\n",
    "        self.conv5 = nn.Conv2d(ch, ch, 3, 1)\n",
    "        self.conv6 = nn.Conv2d(ch, ch, 3, 1)\n",
    "        self.conv7 = nn.Conv2d(ch, ch_out, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return x\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ch_in=64, ch=32, ch_out=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(ch_in, ch, 4, 2)\n",
    "        self.up2 = nn.ConvTranspose2d(ch, ch, 4, 2)\n",
    "        self.up3 = nn.ConvTranspose2d(ch, ch, 4, 2)\n",
    "        self.up4 = nn.ConvTranspose2d(ch, ch, 4, 2)\n",
    "        self.up5 = nn.ConvTranspose2d(ch, ch, 4, 2)\n",
    "        self.up6 = nn.ConvTranspose2d(ch, ch, 4, 2)\n",
    "        self.up7 = nn.ConvTranspose2d(ch, ch_out, 4, 2)\n",
    "\n",
    "    def forward(self, x0):\n",
    "\n",
    "        x = self.up1(x0)\n",
    "        x1 = self.relu(x)\n",
    "\n",
    "        x = self.up2(x1)\n",
    "        x2 = self.relu(x)\n",
    "\n",
    "        x = self.up3(x2)\n",
    "        x3 = self.relu(x)\n",
    "\n",
    "        x = self.up4(x3)\n",
    "        x4 = self.relu(x)\n",
    "\n",
    "        x = self.up5(x4)\n",
    "        x5 = self.relu(x)\n",
    "\n",
    "        x = self.up6(x5)\n",
    "        x6 = self.relu(x)\n",
    "\n",
    "        x = self.up7(x6)\n",
    "\n",
    "        x = F.interpolate(x, (IMG_SIZE, IMG_SIZE), mode=\"bilinear\")\n",
    "\n",
    "        x = nn.Tanh()(x)\n",
    "        \n",
    "        return x\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(ch_in=3, ch=64, ch_out=128)\n",
    "        self.decoder = Decoder(ch_in=128, ch=64, ch_out=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.latent = self.encoder(x)\n",
    "        x = self.decoder(self.latent)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790f130-622d-495a-962d-bc4ebcecaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964393f-5561-4b5a-a14f-694bfe39debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in model.encoder.parameters() if p.requires_grad))\n",
    "print(sum(p.numel() for p in model.decoder.parameters() if p.requires_grad))\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcda3f-6e3e-4369-b87a-e0a979e9c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96862f-556e-4742-8d66-f25610bc4611",
   "metadata": {},
   "outputs": [],
   "source": [
    "element = next(iter(data_loader))\n",
    "\n",
    "x = element[\"img\"]\n",
    "x = x[0:1]\n",
    "x = x.to(device)\n",
    "print(x.shape, x.numel())\n",
    "\n",
    "latent = model.encoder(x)\n",
    "print(latent.shape, latent.numel())\n",
    "print(\"Compression: \", x.numel() / latent.numel())\n",
    "y = model.decoder(latent)\n",
    "print(y.shape, y.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c9a3f-22a2-49d4-8aa1-95311f35cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "element = next(iter(data_loader))\n",
    "images = element[\"img\"]\n",
    "show(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9b658-a296-4ea5-8cf8-ad24a20c4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(epoch, num_epochs - epoch):\n",
    "    losses = list()\n",
    "    iterator = iter(data_loader)\n",
    "    for batch in range(len(data_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            for i in range(N_BS):\n",
    "                element = next(iterator)\n",
    "                images = element[\"img\"]\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    y = model(images.to(device))\n",
    "                    loss = criterion(y, images.to(device))\n",
    "                    losses.append(loss.item())\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "        except Exception as e:\n",
    "            iterator = iter(data_loader)\n",
    "            \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if (batch + 1) % 500 == 0:\n",
    "            show(images)\n",
    "\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f'\\rEpoch [{epoch + 1}/{num_epochs}], Batch [{batch + 1}/{len(data_loader)}], loss: {np.mean(losses):.6f}', end=\"\")\n",
    "            losses = list()\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        show(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c3c5e-370f-452f-94e8-6cdb02971a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
