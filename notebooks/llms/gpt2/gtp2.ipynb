{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f086031db13af63f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:08.689980700Z",
     "start_time": "2024-04-15T15:22:08.168135100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "import inspect\n",
    "import json\n",
    "import mlflow\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d41239f313e3a36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:08.728069400Z",
     "start_time": "2024-04-15T15:22:08.719098300Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 56\n",
    "BATCH_ACC = 70\n",
    "NUM_WORKERS = 24\n",
    "DEVICE = \"cuda\"\n",
    "TOKENIZER_MODEL = \"gpt2\"\n",
    "N_EPOCHS = 100\n",
    "LR = 1e-4\n",
    "FINE_TUNE = False\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(TOKENIZER_MODEL)\n",
    "VOCAB_SIZE = tokenizer.n_vocab\n",
    "END_TOKEN = VOCAB_SIZE - 1\n",
    "\n",
    "BLOCK_SIZE = 256\n",
    "N_EMBD = 768\n",
    "N_HEAD = 12\n",
    "N_LAYER = 12\n",
    "DROPOUT = 0.1\n",
    "R = 2\n",
    "\n",
    "FILE_NAME = f\"checkpoints/gpt2_{FINE_TUNE}_{BLOCK_SIZE}_{N_EMBD}_{N_LAYER}_{N_HEAD}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e87aeec7c6ccdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:08.737990600Z",
     "start_time": "2024-04-15T15:22:08.719098300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Start training from new Model\n",
    "# FINE_TUNE = False\n",
    "# CHECKPOINT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a1d6fa69ae1e70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:10.138540Z",
     "start_time": "2024-04-15T15:22:08.720486Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Start training from a checkpoint\n",
    "FINE_TUNE = False\n",
    "CHECKPOINT = torch.load(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec789b4ed906026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:10.152798100Z",
     "start_time": "2024-04-15T15:22:10.152798100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Start fine tuning the other parameters\n",
    "# FINE_TUNE = True\n",
    "# CHECKPOINT = torch.load(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ef2e30-647f-4174-9151-5efce7ef4427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:10.338948100Z",
     "start_time": "2024-04-15T15:22:10.326927800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = BLOCK_SIZE\n",
    "    vocab_size: int = VOCAB_SIZE\n",
    "    batch_acc: int = BATCH_ACC\n",
    "    n_layer: int = N_LAYER\n",
    "    n_head: int = N_HEAD\n",
    "    n_embd: int = N_EMBD\n",
    "    dropout: float = DROPOUT\n",
    "    bias: bool = False\n",
    "    r: int = R\n",
    "    lr: float = LR\n",
    "    checkpoint = CHECKPOINT\n",
    "    fine_tune = FINE_TUNE\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, block_size, train=True):\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        self.elements = []\n",
    "        self.elements.extend(glob.glob(\"data/wikipedia/tokens/*\"))\n",
    "        self.elements.extend(glob.glob(\"data/tatsu_lab_alpaca/tokens/*\"))\n",
    "        self.elements.extend(glob.glob(\"data/truthful_qa/tokens/*\"))\n",
    "        self.elements.extend(glob.glob(\"data/open_orca/tokens/*\"))\n",
    "        self.elements.extend(glob.glob(\"data/stingning_ultrachat/tokens/*\"))\n",
    "        self.elements.extend(glob.glob(\"data/xtreme/tokens/*\"))\n",
    "        self.elements.extend(glob.glob(\"data/openwebtext/tokens/*\"))\n",
    "        self.elements = np.asarray(self.elements)\n",
    "\n",
    "        n_train = int(len(self) * 0.9995)\n",
    "\n",
    "        if train:\n",
    "            self.elements = self.elements[:n_train]\n",
    "        else:\n",
    "            self.elements = self.elements[n_train:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.elements)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        tokens = []\n",
    "        while len(tokens) <= self.block_size + 1:\n",
    "            filename = self.elements[idx]\n",
    "            with open(filename, \"rb\") as f:\n",
    "                tokens = tokens + json.load(f)[\"tokens\"]\n",
    "            idx = np.random.randint(0, self.block_size + 1)\n",
    "\n",
    "        start_idx = np.random.randint(0, max(1, len(tokens) - (self.block_size + 1)))\n",
    "        \n",
    "        tokens = np.array(tokens)\n",
    "        tokens = tokens[start_idx:start_idx + self.block_size + 1]\n",
    "        tokens = torch.from_numpy(tokens).to(torch.long)\n",
    "\n",
    "        sample = dict()\n",
    "        sample[\"tokens\"] = tokens\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5e2c16ab6b868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:16.260201Z",
     "start_time": "2024-04-15T15:22:10.338948100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20180136\n",
      "Test: 10096\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(BLOCK_SIZE, train=True)\n",
    "test_dataset = CustomDataset(BLOCK_SIZE, train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=NUM_WORKERS)\n",
    "print(\"Train:\", len(train_dataset))\n",
    "print(\"Test:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922fa794-650b-47b9-bff8-a692be7b9b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([56, 257])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch[\"tokens\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:16.338920500Z",
     "start_time": "2024-04-15T15:22:16.289281500Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "class LoRACausalSelfAttention(nn.Module):\n",
    "    def __init__(self, attn, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.c_attn = attn.c_attn\n",
    "        # output projection\n",
    "        self.c_proj = attn.c_proj\n",
    "        # regularization\n",
    "        self.attn_dropout = attn.attn_dropout\n",
    "        self.resid_dropout = attn.resid_dropout\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.bias = att.bias\n",
    "            \n",
    "        self.B = nn.Parameter(torch.zeros([config.n_embd * 3, config.r]), requires_grad=config.fine_tune)\n",
    "        self.A = nn.Parameter(torch.randn([config.r, config.n_embd]), requires_grad=config.fine_tune)\n",
    "        \n",
    "        self.B_proj = nn.Parameter(torch.zeros([config.n_embd, config.r]), requires_grad=config.fine_tune)\n",
    "        self.A_proj = nn.Parameter(torch.randn([config.r, config.n_embd]), requires_grad=config.fine_tune)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        dW = self.B @ self.A\n",
    "        d = x @ dW.T\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = (self.c_attn(x) + d).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        dW_proj = self.B_proj @ self.A_proj\n",
    "        proj = y @ dW_proj.T\n",
    "        \n",
    "        y = self.c_proj(y) + proj\n",
    "        y = self.resid_dropout(y)\n",
    "        \n",
    "        return y\n",
    "class LoRABlock(nn.Module):\n",
    "    def __init__(self, block, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln_1 = block.ln_1\n",
    "        self.attn = LoRACausalSelfAttention(block.attn, config)\n",
    "        self.ln_2 = block.ln_2\n",
    "        self.mlp = block.mlp\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "class LoRAGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.model = GPT(config)\n",
    "            \n",
    "        if config.fine_tune:\n",
    "            print(\"Fine-tuning, set requires_grad = False.\")\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "            \n",
    "        self.lora_blocks = nn.ModuleList([LoRABlock(block, config) for block in self.model.transformer.h])\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.model.transformer.wte(idx)\n",
    "        pos_emb = self.model.transformer.wpe(pos)\n",
    "        x = self.model.transformer.drop(tok_emb + pos_emb)\n",
    "        for lora_block in self.lora_blocks:\n",
    "            x = lora_block(x)\n",
    "        x = self.model.transformer.ln_f(x)\n",
    "        logits = self.model.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        optimizer = self.optimizers(use_pl_optimizer=True)\n",
    "\n",
    "        in_tokens = batch[\"tokens\"][:, :-1].to(DEVICE)\n",
    "        out_tokens = batch[\"tokens\"][:, 1:].to(DEVICE)\n",
    "        logits = self(in_tokens)\n",
    "        loss = F.cross_entropy(logits.reshape(logits.shape[0] * BLOCK_SIZE, -1), out_tokens.reshape(-1))\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        return self.model.generate(idx, max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2debb6c4a557c191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:19.216662800Z",
     "start_time": "2024-04-15T15:22:16.358594200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 123.747.840\n",
      "LoRA: 123.747.840\n",
      "ratio: 100.00%\n",
      "\n",
      "torch.Size([56, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/17 11:02:44 INFO mlflow.tracking.fluent: Experiment with name 'GPT2 - fine_tune=False, block_size=256, n_embd=768, n_layer=12, n_head=12' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([56, 256, 50257])\n",
      "tensor(3.1405, device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "model = LoRAGPT(config).to(DEVICE)\n",
    "\n",
    "if config.checkpoint is not None:\n",
    "    model.model.load_state_dict(config.checkpoint[\"model_state_dict\"])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "if config.checkpoint is not None:\n",
    "    scaler.load_state_dict(config.checkpoint[\"scaler_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "if config.checkpoint is not None:\n",
    "    optimizer.load_state_dict(config.checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "n_model_parameters = sum(p.numel() for p in model.model.parameters())\n",
    "n_lora_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {n_model_parameters:,}\".replace(\",\", \".\"))\n",
    "print(f\"LoRA: {n_lora_parameters:,}\".replace(\",\", \".\"))\n",
    "print(f\"ratio: {(n_lora_parameters / n_model_parameters) * 100:.02f}%\")\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(train_loader))\n",
    "    in_tokens = batch[\"tokens\"][:, :-1].to(DEVICE).to(torch.long)\n",
    "    out_tokens = batch[\"tokens\"][:, 1:].to(DEVICE).to(torch.long)\n",
    "    print(in_tokens.shape)\n",
    "    logits = model(in_tokens)\n",
    "    print(logits.shape)\n",
    "    a = logits.view(logits.shape[0] * BLOCK_SIZE, -1)\n",
    "    b = out_tokens.view(-1)\n",
    "    loss = F.cross_entropy(a, b)\n",
    "    print(loss)\n",
    "    print()\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")\n",
    "_ = mlflow.set_experiment(f\"GPT2 - fine_tune={FINE_TUNE}, block_size={BLOCK_SIZE}, n_embd={N_EMBD}, n_layer={N_LAYER}, n_head={N_HEAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c00242d9b3e552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:19.223374700Z",
     "start_time": "2024-04-15T15:22:19.216662800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_examples(loader, num_new_tokens=16, verbose=False):\n",
    "    data = dict()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(loader))\n",
    "\n",
    "        in_tokens = batch[\"tokens\"][:, :-1].to(DEVICE)\n",
    "        out_tokens = batch[\"tokens\"][:, 1:].to(DEVICE)\n",
    "\n",
    "        pred = model.generate(in_tokens, max_new_tokens=num_new_tokens)\n",
    "\n",
    "        for i, (in_tokens_, out_tokens_, pred_) in enumerate(zip(in_tokens, out_tokens, pred)):\n",
    "            in_tokens_ = in_tokens_.detach().cpu().numpy()\n",
    "            out_tokens_ = out_tokens_.detach().cpu().numpy()\n",
    "            pred_ = pred_.detach().cpu().numpy()\n",
    "            pred_ = pred_[:-num_new_tokens].tolist() + [VOCAB_SIZE - 1] + pred_[-num_new_tokens:].tolist()\n",
    "\n",
    "            in_text = tokenizer.decode(in_tokens_)\n",
    "            pred_text = tokenizer.decode(pred_)\n",
    "\n",
    "            data[i] = {\"in_text\": in_text, \"pred_text\": pred_text}\n",
    "\n",
    "            if verbose:\n",
    "                print(\"INPUT\")\n",
    "                print(in_text)\n",
    "                print(\"=========================================\")\n",
    "                print(\"OUTPUT\")\n",
    "                print(pred_text)\n",
    "                print(\"=========================================\")\n",
    "                print(\"=========================================\")\n",
    "                print(\"=========================================\")\n",
    "                print()\n",
    "    return data\n",
    "def train():\n",
    "    iterator = iter(train_loader)\n",
    "    N = len(train_loader) // BATCH_SIZE\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    for n_epoch in range(N_EPOCHS):\n",
    "        for n in range(N):\n",
    "            for b in range(BATCH_ACC):\n",
    "                with torch.autocast(device_type=DEVICE, dtype=torch.float16, enabled=True):\n",
    "                    batch = next(iterator)\n",
    "                    \n",
    "                    in_tokens = batch[\"tokens\"][:, :-1].to(DEVICE)\n",
    "                    out_tokens = batch[\"tokens\"][:, 1:].to(DEVICE)\n",
    "                    \n",
    "                    logits = model(in_tokens)\n",
    "                    loss = F.cross_entropy(logits.view(logits.shape[0] * BLOCK_SIZE, -1), out_tokens.view(-1), ignore_index=END_TOKEN)\n",
    "                    mlflow.log_metric(\"train_loss\", loss, step=count, synchronous=False)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                sum_loss += loss.item()\n",
    "                count += 1\n",
    "\n",
    "                mlflow.log_metric(\"mean_train_loss\", sum_loss / count, step=(n_epoch + 1) * (n + 1) * BATCH_ACC, synchronous=False)\n",
    "\n",
    "                print(f\"\\r{n_epoch + 1:03d}|{N_EPOCHS:03d}, {n + 1:04d}|{N:04d}, {b + 1:03d}|{BATCH_ACC:03d}, loss: {sum_loss / count:.05f}\", end=\"\")\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "            if (n + 1) % 25 == 0:\n",
    "                print()\n",
    "                sum_loss = 0\n",
    "                count = 0\n",
    "                \n",
    "                \n",
    "            if (n + 1) % 150 == 0:\n",
    "                print(\"\\nSave...\")\n",
    "                torch.save({\"model_state_dict\": model.model.state_dict(), \n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(), \n",
    "                            \"scaler_state_dict\": scaler.state_dict()}, FILE_NAME)\n",
    "                print(\"...done!\\n\")\n",
    "                data = generate_examples(train_loader, num_new_tokens=16, verbose=False)\n",
    "                mlflow.log_dict(data, f\"example_{n_epoch + 1}_{n + 1}.json\")\n",
    "                test(n_epoch + 1, (n_epoch + 1) * (n + 1) * BATCH_ACC)\n",
    "@torch.no_grad\n",
    "def test(epoch=0, step=0):\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        in_tokens = batch[\"tokens\"][:, :-1].to(DEVICE)\n",
    "        out_tokens = batch[\"tokens\"][:, 1:].to(DEVICE)\n",
    "        \n",
    "        logits = model(in_tokens)\n",
    "        loss = F.cross_entropy(logits.view(logits.shape[0] * BLOCK_SIZE, -1), out_tokens.view(-1), ignore_index=END_TOKEN)\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "        print(f\"\\r{i + 1:04d}|{len(test_loader):04d}, loss: {sum_loss / count:.05f}\", end=\"\")\n",
    "\n",
    "    data = generate_examples(test_loader, num_new_tokens=16, verbose=False)\n",
    "    mlflow.log_dict(data, f\"example_{epoch}_{i + 1}.json\")\n",
    "    mlflow.log_metric(\"test_loss\", sum_loss / count, step=step, synchronous=False)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "801afe68-e292-4b61-9eb1-886dbb72bb7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001|100, 0025|6435, 070|070, loss: 3.20612\n",
      "001|100, 0050|6435, 070|070, loss: 3.19906\n",
      "001|100, 0075|6435, 070|070, loss: 3.18227\n",
      "001|100, 0100|6435, 070|070, loss: 3.17905\n",
      "001|100, 0125|6435, 070|070, loss: 3.16608\n",
      "001|100, 0150|6435, 070|070, loss: 3.15661\n",
      "\n",
      "Save...\n",
      "...done!\n",
      "\n",
      "0181|0181, loss: 4.33198\n",
      "\n",
      "001|100, 0175|6435, 070|070, loss: 3.14565\n",
      "001|100, 0200|6435, 070|070, loss: 3.13403\n",
      "001|100, 0225|6435, 070|070, loss: 3.12882\n",
      "001|100, 0250|6435, 070|070, loss: 3.11632\n",
      "001|100, 0275|6435, 070|070, loss: 3.10877\n",
      "001|100, 0300|6435, 070|070, loss: 3.10169\n",
      "\n",
      "Save...\n",
      "...done!\n",
      "\n",
      "0181|0181, loss: 4.27377\n",
      "\n",
      "001|100, 0325|6435, 070|070, loss: 3.08980\n",
      "001|100, 0350|6435, 070|070, loss: 3.08763\n",
      "001|100, 0375|6435, 070|070, loss: 3.07854\n",
      "001|100, 0400|6435, 070|070, loss: 3.07325\n",
      "001|100, 0425|6435, 070|070, loss: 3.06310\n",
      "001|100, 0450|6435, 070|070, loss: 3.06527\n",
      "\n",
      "Save...\n",
      "...done!\n",
      "\n",
      "0181|0181, loss: 4.22408\n",
      "\n",
      "001|100, 0475|6435, 070|070, loss: 3.05941\n",
      "001|100, 0500|6435, 070|070, loss: 3.05703\n",
      "001|100, 0525|6435, 070|070, loss: 3.04683\n",
      "001|100, 0550|6435, 070|070, loss: 3.04059\n",
      "001|100, 0575|6435, 070|070, loss: 3.03444\n",
      "001|100, 0600|6435, 070|070, loss: 3.02693\n",
      "\n",
      "Save...\n",
      "...done!\n",
      "\n",
      "0181|0181, loss: 4.18582\n",
      "\n",
      "001|100, 0625|6435, 070|070, loss: 3.02531\n",
      "001|100, 0650|6435, 070|070, loss: 3.01297\n",
      "001|100, 0675|6435, 070|070, loss: 3.00969\n",
      "001|100, 0700|6435, 070|070, loss: 3.00540\n",
      "001|100, 0725|6435, 070|070, loss: 3.00172\n",
      "001|100, 0750|6435, 070|070, loss: 3.00533\n",
      "\n",
      "Save...\n",
      "...done!\n",
      "\n",
      "0181|0181, loss: 4.15184\n",
      "\n",
      "001|100, 0752|6435, 017|070, loss: 2.97325"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_id\u001b[38;5;241m=\u001b[39mrun_id):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, step\u001b[38;5;241m=\u001b[39mcount, synchronous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 52\u001b[0m sum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     55\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_train_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, sum_loss \u001b[38;5;241m/\u001b[39m count, step\u001b[38;5;241m=\u001b[39m(n_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m (n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m BATCH_ACC, synchronous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_id = \"8b22f338f6cf4b80981c29a3e005377e\"#None\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97db292-d441-414e-8623-c3d6ad1e4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.model.state_dict(), \n",
    "            \"optimizer_state_dict\": optimizer.state_dict(), \n",
    "            \"scaler_state_dict\": scaler.state_dict()}, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d554208-3979-43bb-b403-c4e5123c10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_examples(train_loader, num_new_tokens=16, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28cd31-370d-4dc4-8609-6c47e4a7bf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
