{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f086031db13af63f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:08.689980700Z",
     "start_time": "2024-04-15T15:22:08.168135100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "import inspect\n",
    "import json\n",
    "import mlflow\n",
    "import tiktoken\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d41239f313e3a36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:08.728069400Z",
     "start_time": "2024-04-15T15:22:08.719098300Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "BATCH_ACC = 70\n",
    "NUM_WORKERS = 16\n",
    "DEVICE = \"cuda\"\n",
    "TOKENIZER_MODEL = \"gpt2\"\n",
    "N_EPOCHS = 100\n",
    "LR = 1e-4\n",
    "FINE_TUNE = False\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(TOKENIZER_MODEL)\n",
    "VOCAB_SIZE = tokenizer.n_vocab\n",
    "END_TOKEN = VOCAB_SIZE - 1\n",
    "\n",
    "BLOCK_SIZE = 256\n",
    "N_EMBD = 768\n",
    "N_HEAD = 12\n",
    "N_LAYER = 12\n",
    "DROPOUT = 0.1\n",
    "R = 16\n",
    "WINDOW = 1\n",
    "\n",
    "ROOT_PATH = \"/mnt\"\n",
    "FILE_NAME = f\"/mnt/data/checkpoints/gpt2_alibi_{FINE_TUNE}_{BLOCK_SIZE}_{N_EMBD}_{N_LAYER}_{N_HEAD}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e87aeec7c6ccdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:08.737990600Z",
     "start_time": "2024-04-15T15:22:08.719098300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Start training from new Model\n",
    "# FINE_TUNE = False\n",
    "# CHECKPOINT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a1d6fa69ae1e70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:10.138540Z",
     "start_time": "2024-04-15T15:22:08.720486Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Start training from a checkpoint\n",
    "FINE_TUNE = False\n",
    "CHECKPOINT = torch.load(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec789b4ed906026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:10.152798100Z",
     "start_time": "2024-04-15T15:22:10.152798100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Start fine tuning the other parameters\n",
    "# FINE_TUNE = True\n",
    "# CHECKPOINT = torch.load(FILE_NAME)\n",
    "# BLOCK_SIZE = BLOCK_SIZE * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ef2e30-647f-4174-9151-5efce7ef4427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:10.338948100Z",
     "start_time": "2024-04-15T15:22:10.326927800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = BLOCK_SIZE\n",
    "    vocab_size: int = VOCAB_SIZE\n",
    "    batch_acc: int = BATCH_ACC\n",
    "    n_layer: int = N_LAYER\n",
    "    n_head: int = N_HEAD\n",
    "    n_embd: int = N_EMBD\n",
    "    dropout: float = DROPOUT\n",
    "    bias: bool = False\n",
    "    r: int = R\n",
    "    lr: float = LR\n",
    "    checkpoint = CHECKPOINT\n",
    "    fine_tune = FINE_TUNE\n",
    "    window = WINDOW\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, block_size, window, train=True):\n",
    "        self.root = root\n",
    "        self.block_size = block_size\n",
    "        self.window = window\n",
    "        \n",
    "        self.elements = []\n",
    "        self.elements.extend(glob.glob(os.path.join(self.root, \"data/wikipedia/tokens/*\")))\n",
    "        self.elements.extend(glob.glob(os.path.join(self.root, \"data/tatsu_lab_alpaca/tokens/*\")))\n",
    "        self.elements.extend(glob.glob(os.path.join(self.root, \"data/truthful_qa/tokens/*\")))\n",
    "        self.elements.extend(glob.glob(os.path.join(self.root, \"data/open_orca/tokens/*\")))\n",
    "        self.elements.extend(glob.glob(os.path.join(self.root, \"data/stingning_ultrachat/tokens/*\")))\n",
    "        self.elements.extend(glob.glob(os.path.join(self.root, \"data/xtreme/tokens/*\")))\n",
    "        self.elements.extend(glob.glob(os.path.join(self.root, \"data/openwebtext/tokens/*\")))\n",
    "        self.elements.extend(glob.glob(os.path.join(self.root, \"data/oscar/tokens/*\")))\n",
    "        self.elements.extend(glob.glob(os.path.join(self.root, \"data/minipile/tokens/*\")))\n",
    "        self.elements.extend(glob.glob(os.path.join(self.root, \"data/cc100/tokens/*\")))\n",
    "        self.elements = np.asarray(self.elements)\n",
    "\n",
    "        n_train = int(len(self) * 0.9995)\n",
    "\n",
    "        if train:\n",
    "            self.elements = self.elements[:n_train]\n",
    "        else:\n",
    "            self.elements = self.elements[n_train:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.elements)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        tokens = []\n",
    "        while len(tokens) <= self.block_size + self.window:\n",
    "            filename = self.elements[idx]\n",
    "            with open(filename, \"rb\") as f:\n",
    "                tokens = tokens + json.load(f)[\"tokens\"]\n",
    "            idx = np.random.randint(0, self.block_size + self.window)\n",
    "\n",
    "        start_idx = np.random.randint(0, max(1, len(tokens) - (self.block_size + self.window)))\n",
    "        \n",
    "        tokens = np.array(tokens)\n",
    "        tokens = tokens[start_idx:start_idx + self.block_size + self.window]\n",
    "        tokens = torch.from_numpy(tokens).to(torch.long)\n",
    "\n",
    "        sample = dict()\n",
    "        sample[\"tokens\"] = tokens\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5e2c16ab6b868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:16.260201Z",
     "start_time": "2024-04-15T15:22:10.338948100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20180136\n",
      "Test: 10096\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(ROOT_PATH, BLOCK_SIZE, WINDOW, train=True)\n",
    "test_dataset = CustomDataset(ROOT_PATH, BLOCK_SIZE, WINDOW, train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=NUM_WORKERS)\n",
    "print(\"Train:\", len(train_dataset))\n",
    "print(\"Test:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922fa794-650b-47b9-bff8-a692be7b9b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 257])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch[\"tokens\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:16.338920500Z",
     "start_time": "2024-04-15T15:22:16.289281500Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config, h):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.config = config\n",
    "        self.config.h = h\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        n = config.block_size\n",
    "        self.attn_mask = torch.cat([torch.cat([torch.linspace(-i, 0, i + 1), torch.zeros(n - i - 1)])[None, :] for i in range(n)]) / (n - 1) * (1 / 2**self.config.h)\n",
    "        self.bias = torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        device = x.device\n",
    "\n",
    "        self.attn_mask = torch.cat([torch.cat([torch.linspace(-i, 0, i + 1), torch.zeros(T - i - 1)])[None, :] for i in range(T)]) / (T - 1) * (1 / 2**self.config.h)\n",
    "        self.bias = torch.tril(torch.ones(T, T)).view(1, 1, T, T)\n",
    "\n",
    "        self.bias = self.bias.to(device)\n",
    "        self.attn_mask = self.attn_mask.to(device)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = att + self.attn_mask.to(device)\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config, h):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config, h)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        f = (8 / self.config.n_layer)\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config, h + f) for h in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = self.transformer.drop(tok_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -self.config.window, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-self.config.window]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "class LoRACausalSelfAttention(nn.Module):\n",
    "    def __init__(self, attn, config, h):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.config = config\n",
    "        self.config.h = h\n",
    "        self.c_attn = attn.c_attn\n",
    "        # output projection\n",
    "        self.c_proj = attn.c_proj\n",
    "        # regularization\n",
    "        self.attn_dropout = attn.attn_dropout\n",
    "        self.resid_dropout = attn.resid_dropout\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.bias = attn.bias\n",
    "        self.attn_mask = attn.attn_mask\n",
    "            \n",
    "        self.B = nn.Parameter(torch.zeros([config.n_embd * 3, config.r]), requires_grad=config.fine_tune)\n",
    "        self.A = nn.Parameter(torch.randn([config.r, config.n_embd]), requires_grad=config.fine_tune)\n",
    "        \n",
    "        self.B_proj = nn.Parameter(torch.zeros([config.n_embd, config.r]), requires_grad=config.fine_tune)\n",
    "        self.A_proj = nn.Parameter(torch.randn([config.r, config.n_embd]), requires_grad=config.fine_tune)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        device = x.device\n",
    "\n",
    "        dW = self.B @ self.A\n",
    "        d = x @ dW.T\n",
    "\n",
    "        self.attn_mask = torch.cat([torch.cat([torch.linspace(-i, 0, i + 1), torch.zeros(T - i - 1)])[None, :] for i in range(T)]) / (T - 1) * (1 / 2**self.config.h)\n",
    "        self.bias = torch.tril(torch.ones(T, T)).view(1, 1, T, T)\n",
    "\n",
    "        self.bias = self.bias.to(device)\n",
    "        self.attn_mask = self.attn_mask.to(device)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = (self.c_attn(x) + d).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = att + self.attn_mask\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        dW_proj = self.B_proj @ self.A_proj\n",
    "        proj = y @ dW_proj.T\n",
    "        \n",
    "        y = self.c_proj(y) + proj\n",
    "        y = self.resid_dropout(y)\n",
    "        \n",
    "        return y\n",
    "class LoRABlock(nn.Module):\n",
    "    def __init__(self, block, config, h):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln_1 = block.ln_1\n",
    "        self.attn = LoRACausalSelfAttention(block.attn, config, h)\n",
    "        self.ln_2 = block.ln_2\n",
    "        self.mlp = block.mlp\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "class LoRAGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.model = GPT(config)\n",
    "            \n",
    "        if config.fine_tune:\n",
    "            print(\"Fine-tuning, set requires_grad = False.\")\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        f = (8 / self.config.n_layer)\n",
    "        self.lora_blocks = nn.ModuleList([LoRABlock(block, config, h + f) for h, block in enumerate(self.model.transformer.h)])\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.model.transformer.wte(idx)\n",
    "        x = self.model.transformer.drop(tok_emb)\n",
    "        for lora_block in self.lora_blocks:\n",
    "            x = lora_block(x)\n",
    "        x = self.model.transformer.ln_f(x)\n",
    "        logits = self.model.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        return self.model.generate(idx, max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2debb6c4a557c191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:19.216662800Z",
     "start_time": "2024-04-15T15:22:16.358594200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 123.551.232\n",
      "LoRA: 123.551.232\n",
      "ratio: 100.00%\n",
      "\n",
      "torch.Size([48, 256])\n",
      "torch.Size([48, 256, 50257])\n",
      "tensor(7.3060, device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "model = LoRAGPT(config).to(DEVICE)\n",
    "\n",
    "if config.checkpoint is not None:\n",
    "    model.model.load_state_dict(config.checkpoint[\"model_state_dict\"])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "if config.checkpoint is not None:\n",
    "    scaler.load_state_dict(config.checkpoint[\"scaler_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "if config.checkpoint is not None:\n",
    "    optimizer.load_state_dict(config.checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "n_model_parameters = sum(p.numel() for p in model.model.parameters())\n",
    "n_lora_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {n_model_parameters:,}\".replace(\",\", \".\"))\n",
    "print(f\"LoRA: {n_lora_parameters:,}\".replace(\",\", \".\"))\n",
    "print(f\"ratio: {(n_lora_parameters / n_model_parameters) * 100:.02f}%\")\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(train_loader))\n",
    "    in_tokens = batch[\"tokens\"][:, :-WINDOW].to(DEVICE).to(torch.long)\n",
    "    out_tokens = batch[\"tokens\"][:, WINDOW:].to(DEVICE).to(torch.long)\n",
    "    print(in_tokens.shape)\n",
    "    logits = model(in_tokens)\n",
    "    print(logits.shape)\n",
    "    a = logits.view(logits.shape[0] * BLOCK_SIZE, -1)\n",
    "    b = out_tokens.view(-1)\n",
    "    loss = F.cross_entropy(a, b)\n",
    "    print(loss)\n",
    "    print()\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")\n",
    "_ = mlflow.set_experiment(f\"GPT2 - ALiBi - fine_tune={FINE_TUNE}, block_size={BLOCK_SIZE}, n_embd={N_EMBD}, n_layer={N_LAYER}, n_head={N_HEAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c00242d9b3e552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:19.223374700Z",
     "start_time": "2024-04-15T15:22:19.216662800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_examples(loader, num_new_tokens=16, verbose=False):\n",
    "    data = dict()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(loader))\n",
    "\n",
    "        in_tokens = batch[\"tokens\"][:, :-1].to(DEVICE)\n",
    "        out_tokens = batch[\"tokens\"][:, 1:].to(DEVICE)\n",
    "\n",
    "        pred = model.generate(in_tokens, max_new_tokens=num_new_tokens)\n",
    "\n",
    "        for i, (in_tokens_, out_tokens_, pred_) in enumerate(zip(in_tokens, out_tokens, pred)):\n",
    "            in_tokens_ = in_tokens_.detach().cpu().numpy()\n",
    "            out_tokens_ = out_tokens_.detach().cpu().numpy()\n",
    "            pred_ = pred_.detach().cpu().numpy()\n",
    "            pred_ = pred_[:-num_new_tokens].tolist() + [VOCAB_SIZE - 1] + pred_[-num_new_tokens:].tolist()\n",
    "\n",
    "            in_text = tokenizer.decode(in_tokens_)\n",
    "            pred_text = tokenizer.decode(pred_)\n",
    "\n",
    "            data[i] = {\"in_text\": in_text, \"pred_text\": pred_text}\n",
    "\n",
    "            if verbose:\n",
    "                print(\"INPUT\")\n",
    "                print(in_text)\n",
    "                print(\"=========================================\")\n",
    "                print(\"OUTPUT\")\n",
    "                print(pred_text)\n",
    "                print(\"=========================================\")\n",
    "                print(\"=========================================\")\n",
    "                print(\"=========================================\")\n",
    "                print()\n",
    "    return data\n",
    "def train():\n",
    "    iterator = iter(train_loader)\n",
    "    N = len(train_loader) // BATCH_SIZE\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    for n_epoch in range(N_EPOCHS):\n",
    "        for n in range(N):\n",
    "            for b in range(BATCH_ACC):\n",
    "                with torch.autocast(device_type=DEVICE, dtype=torch.float16, enabled=True):\n",
    "                    batch = next(iterator)\n",
    "                    \n",
    "                    in_tokens = batch[\"tokens\"][:, :-WINDOW].to(DEVICE)\n",
    "                    out_tokens = batch[\"tokens\"][:, WINDOW:].to(DEVICE)\n",
    "                    \n",
    "                    logits = model(in_tokens)\n",
    "                    loss = F.cross_entropy(logits.view(logits.shape[0] * BLOCK_SIZE, -1), out_tokens.view(-1), ignore_index=END_TOKEN)\n",
    "                    mlflow.log_metric(\"train_loss\", loss, step=count, synchronous=False)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                sum_loss += loss.item()\n",
    "                count += 1\n",
    "\n",
    "                mlflow.log_metric(\"mean_train_loss\", sum_loss / count, step=(n_epoch + 1) * (n + 1) * BATCH_ACC, synchronous=False)\n",
    "\n",
    "                print(f\"\\r{n_epoch + 1:03d}|{N_EPOCHS:03d}, {n + 1:04d}|{N:04d}, {b + 1:03d}|{BATCH_ACC:03d}, loss: {sum_loss / count:.05f}\", end=\"\")\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "            if (n + 1) % 25 == 0:\n",
    "                print()\n",
    "                sum_loss = 0\n",
    "                count = 0\n",
    "                \n",
    "            if (n + 1) % 150 == 0:\n",
    "                print(\"\\nSave...\")\n",
    "                torch.save({\"model_state_dict\": model.model.state_dict(), \n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(), \n",
    "                            \"scaler_state_dict\": scaler.state_dict()}, FILE_NAME)\n",
    "                print(\"...done!\\n\")\n",
    "                data = generate_examples(train_loader, num_new_tokens=16, verbose=False)\n",
    "                mlflow.log_dict(data, f\"example_{n_epoch + 1}_{n + 1}.json\")\n",
    "                test(n_epoch + 1, (n_epoch + 1) * (n + 1) * BATCH_ACC)\n",
    "@torch.no_grad\n",
    "def test(epoch=0, step=0):\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        in_tokens = batch[\"tokens\"][:, :-WINDOW].to(DEVICE)\n",
    "        out_tokens = batch[\"tokens\"][:, WINDOW:].to(DEVICE)\n",
    "        \n",
    "        logits = model(in_tokens)\n",
    "        loss = F.cross_entropy(logits.view(logits.shape[0] * BLOCK_SIZE, -1), out_tokens.view(-1), ignore_index=END_TOKEN)\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "        print(f\"\\r{i + 1:04d}|{len(test_loader):04d}, loss: {sum_loss / count:.05f}\", end=\"\")\n",
    "\n",
    "    data = generate_examples(test_loader, num_new_tokens=16, verbose=False)\n",
    "    mlflow.log_dict(data, f\"example_{epoch}_{i + 1}.json\")\n",
    "    mlflow.log_metric(\"test_loss\", sum_loss / count, step=step, synchronous=False)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801afe68-e292-4b61-9eb1-886dbb72bb7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001|100, 0025|8758, 070|070, loss: 6.26017\n",
      "001|100, 0050|8758, 070|070, loss: 5.60162\n",
      "001|100, 0059|8758, 065|070, loss: 5.32654"
     ]
    }
   ],
   "source": [
    "run_id = \"06ec1fd7920d49b6968eeebf180f97d6\"\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97db292-d441-414e-8623-c3d6ad1e4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.model.state_dict(), \n",
    "            \"optimizer_state_dict\": optimizer.state_dict(), \n",
    "            \"scaler_state_dict\": scaler.state_dict()}, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d554208-3979-43bb-b403-c4e5123c10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_examples(train_loader, num_new_tokens=16, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc84b25-2de9-44bc-8f78-6ba357805e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
