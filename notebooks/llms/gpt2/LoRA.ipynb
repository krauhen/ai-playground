{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:16:35.330123400Z",
     "start_time": "2024-04-15T11:16:35.312716600Z"
    }
   },
   "id": "d221af9ec879778e"
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:16:35.547642800Z",
     "start_time": "2024-04-15T11:16:35.522006900Z"
    }
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        emb = tok_emb + pos_emb\n",
    "        emb = self.dropout(emb)\n",
    "        return emb\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            \"embedding\": Embedding(vocab_size, block_size, n_embd, dropout),\n",
    "            \"blocks\": nn.ModuleDict({f\"block_{i}\": Block(n_embd, n_head, block_size, dropout) for i in range(n_layer)}),\n",
    "            \"ln_f\": nn.LayerNorm(n_embd),\n",
    "            \"lm_head\": nn.Linear(n_embd, vocab_size)\n",
    "        })\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        x = self.transformer.embedding(idx)\n",
    "        for name, block in self.transformer.blocks.items():\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.transformer.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [],
   "source": [
    "class LoRAHead(nn.Module):\n",
    "    def __init__(self, head, head_size, n_embd, r):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = head\n",
    "\n",
    "        self.key = head.key\n",
    "        self.query = head.query\n",
    "        self.value = head.value\n",
    "        self.tril = head.tril\n",
    "        self.dropout = head.dropout\n",
    "\n",
    "        self.B_k = nn.Parameter(torch.zeros([head_size, r]), requires_grad=True)\n",
    "        self.A_k = nn.Parameter(torch.randn([r, n_embd]), requires_grad=True)\n",
    "\n",
    "        self.B_q = nn.Parameter(torch.zeros([head_size, r]), requires_grad=True)\n",
    "        self.A_q = nn.Parameter(torch.randn([r, n_embd]), requires_grad=True)\n",
    "\n",
    "        self.B_v = nn.Parameter(torch.zeros([head_size, r]), requires_grad=True)\n",
    "        self.A_v = nn.Parameter(torch.randn([r, n_embd]), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        dW_k = self.B_k @ self.A_k\n",
    "        d_k = x @ dW_k.T\n",
    "        k = self.key(x) + d_k\n",
    "\n",
    "        dW_q = self.B_q @ self.A_q\n",
    "        d_q = x @ dW_q.T\n",
    "        q = self.query(x) + d_q\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        dW_v = self.B_q @ self.A_v\n",
    "        d_v = x @ dW_v.T\n",
    "        v = self.value(x) + d_v\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "        return out\n",
    "class LoRAMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, sa, num_heads, head_size, n_embd, r):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([LoRAHead(sa.heads[i], head_size, n_embd, r) for i in range(num_heads)])\n",
    "        self.proj = sa.proj\n",
    "        self.dropout = sa.dropout\n",
    "\n",
    "        self.B_proj = nn.Parameter(torch.zeros([n_embd, r]), requires_grad=True)\n",
    "        self.A_proj = nn.Parameter(torch.randn([r, num_heads * head_size]), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        dW_proj = self.B_proj @ self.A_proj\n",
    "        d_proj = out @ dW_proj.T\n",
    "        out = self.proj(out) + d_proj\n",
    "\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "class LoRABlock(nn.Module):\n",
    "    def __init__(self, block, n_embd, n_head, r):\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = LoRAMultiHeadAttention(block.sa, n_head, head_size, n_embd, r)\n",
    "        self.ffwd = block.ffwd\n",
    "        self.ln1 = block.ln1\n",
    "        self.ln2 = block.ln2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "class LoRAModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout, r, state_dict=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = Model(vocab_size, n_embd, block_size, n_head, n_layer, dropout)\n",
    "        # self.model.load_state_dict(state_dict)\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.lora_blocks = nn.ModuleDict({})\n",
    "        for i, (name, block) in enumerate(self.model.transformer.blocks.items()):\n",
    "            self.lora_blocks[f\"lora_block_{i}\"] = LoRABlock(block, n_embd, n_head, r)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        x = self.model.transformer.embedding(idx)\n",
    "        for (name, lora_block) in self.lora_blocks.items():\n",
    "            x = lora_block(x)\n",
    "        x = self.model.transformer.ln_f(x)\n",
    "        logits = self.model.transformer.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        return self.generate(idx, max_new_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:16:35.913645300Z",
     "start_time": "2024-04-15T11:16:35.879612300Z"
    }
   },
   "id": "c146215950298578"
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt2\")\n",
    "VOCAB_SIZE = tokenizer.n_vocab\n",
    "N_EMBD = 128\n",
    "BLOCK_SIZE = 32\n",
    "N_HEAD = 12\n",
    "N_LAYER = 12\n",
    "DROPOUT = 0.01\n",
    "R = 10\n",
    "model = LoRAModel(vocab_size=VOCAB_SIZE, n_embd=N_EMBD, block_size=BLOCK_SIZE, n_head=N_HEAD, n_layer=N_LAYER, dropout=DROPOUT, r=R)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:19:37.297517600Z",
     "start_time": "2024-04-15T11:19:37.035073500Z"
    }
   },
   "id": "c5fda60c1c6c3bf"
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 15.245.905\n",
      "LoRA: 625.920\n",
      "ratio: 4.11%\n"
     ]
    }
   ],
   "source": [
    "n_model_parameters = sum(p.numel() for p in model.model.parameters())\n",
    "n_lora_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {n_model_parameters:,}\".replace(\",\", \".\"))\n",
    "print(f\"LoRA: {n_lora_parameters:,}\".replace(\",\", \".\"))\n",
    "print(F\"ratio: {(n_lora_parameters / n_model_parameters) * 100:.02f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:19:37.468854700Z",
     "start_time": "2024-04-15T11:19:37.450275900Z"
    }
   },
   "id": "d6f92e6039ec8802"
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32])\n",
      "torch.Size([16, 32, 50257])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([16, BLOCK_SIZE], dtype=torch.long)\n",
    "print(x.shape)\n",
    "y = model(x)\n",
    "print(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:19:39.313812600Z",
     "start_time": "2024-04-15T11:19:39.096268100Z"
    }
   },
   "id": "17882b035c75af0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5b0d8682e41528fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
