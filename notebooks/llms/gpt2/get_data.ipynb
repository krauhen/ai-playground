{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T13:37:15.536179800Z",
     "start_time": "2024-04-15T13:37:15.519840800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henning/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import tiktoken\n",
    "import uuid\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1edf984e-1fb0-499d-a3d4-4c55355bcd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/mnt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07659e22fad9621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T13:36:17.571504600Z",
     "start_time": "2024-04-15T13:36:12.926832800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9587b0ab93cb3fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T14:25:08.077703200Z",
     "start_time": "2024-04-15T13:43:26.182943Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_wikipedia():\n",
    "    data_type = \"train\"\n",
    "    directory_name = \"wikipedia\"\n",
    "    dataset = datasets.load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=data_type, cache_dir=\"/mnt/data/.cache/huggingface\")\n",
    "\n",
    "    directory = os.path.join(DATA_PATH, f\"data/{directory_name}\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    M = 128\n",
    "    N = len(dataset) // M\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        data = {\"text\": [], \"tokens\": []}\n",
    "        for j in range(M):\n",
    "            text = dataset[count][\"text\"]\n",
    "            data[\"text\"].append(text)\n",
    "            data[\"tokens\"].append(tokenizer.encode(text))\n",
    "            count += 1\n",
    "\n",
    "            print(f\"\\rget_wikipedia(): {i + 1}|{N}, {j + 1}|{M}, {count=}\", end=\"\")\n",
    "            \n",
    "        with open(os.path.join(DATA_PATH, f\"data/{directory_name}/{data_type}_{count:09d}.json\"), \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c4a300a4abaf6e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_tatsu_lab_alpaca():\n",
    "    data_type = \"train\"\n",
    "    directory_name = \"tatsu_lab_alpaca\"\n",
    "    dataset = datasets.load_dataset(\"tatsu-lab/alpaca\", split=data_type, cache_dir=\"/mnt/data/.cache/huggingface\")\n",
    "\n",
    "    directory = os.path.join(DATA_PATH, f\"data/{directory_name}\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    M = 128\n",
    "    N = len(dataset) // M\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        data = {\"text\": [], \"tokens\": []}\n",
    "        for j in range(M):\n",
    "            text = dataset[count][\"text\"]\n",
    "            data[\"text\"].append(text)\n",
    "            data[\"tokens\"].append(tokenizer.encode(text))\n",
    "            count += 1\n",
    "\n",
    "            print(f\"\\rget_tatsu_lab_alpaca(): {i + 1}|{N}, {j + 1}|{M}, {count=}\", end=\"\")\n",
    "            \n",
    "        with open(os.path.join(DATA_PATH, f\"data/{directory_name}/{data_type}_{count:09d}.json\"), \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633fb2d3-e1ee-441b-838f-0d3c1c37846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truthful_qa_generation():\n",
    "    set_ = \"generation\"\n",
    "    data_type = \"validation\"\n",
    "    directory_name = \"truthful_qa_generation\"\n",
    "    dataset = datasets.load_dataset(\"truthful_qa\", set_, split=data_type, cache_dir=\"/mnt/data/.cache/huggingface\")\n",
    "\n",
    "    directory = os.path.join(DATA_PATH, f\"data/{directory_name}\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    M = 128\n",
    "    N = len(dataset) // M\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        data = {\"text\": [], \"tokens\": []}\n",
    "        for j in range(M):\n",
    "            element = dataset[count]\n",
    "            question = \"Question: \\n\" + element[\"question\"] + \"\\n\\n\"\n",
    "            best_answer = \"Best answer: \\n\" + element[\"best_answer\"] + \"\\n\\n\"\n",
    "            correct_answers = \"Correct answers: \\n\" + \"\\n\".join(element[\"correct_answers\"]) + \"\\n\\n\"\n",
    "            incorrect_answers = \"Incorrect answers: \\n\" + \"\\n\".join(element[\"incorrect_answers\"]) + \"\\n\\n\"\n",
    "            text = question + question + incorrect_answers + best_answer\n",
    "            data[\"text\"].append(text)\n",
    "            data[\"tokens\"].append(tokenizer.encode(text))\n",
    "            count += 1\n",
    "\n",
    "            print(f\"\\rget_truthful_qa_generation(): {i + 1}|{N}, {j + 1}|{M}, {count=}\", end=\"\")\n",
    "            \n",
    "        with open(os.path.join(DATA_PATH, f\"data/{directory_name}/{data_type}_{count:09d}.json\"), \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74c6ec0-79b9-4073-95f5-cb666c3e32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truthful_qa_multiple_choice():\n",
    "    set_ = \"multiple_choice\"\n",
    "    data_type = \"validation\"\n",
    "    directory_name = \"truthful_qa_multiple_choice\"\n",
    "    dataset = datasets.load_dataset(\"truthful_qa\", set_, split=data_type, cache_dir=\"/mnt/data/.cache/huggingface\")\n",
    "\n",
    "    directory = os.path.join(DATA_PATH, f\"data/{directory_name}\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    M = 128\n",
    "    N = len(dataset) // M\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        data = {\"text\": [], \"tokens\": []}\n",
    "        for j in range(M):\n",
    "            element = dataset[count]\n",
    "            question = \"Question: \\n\" + element[\"question\"] + \"\\n\\n\"\n",
    "            choices = \"Choices: \\n\" + \"\\n\".join(element[\"mc1_targets\"][\"choices\"]) + \"\\n\\n\"\n",
    "            idx = np.argmax(element[\"mc1_targets\"][\"labels\"])\n",
    "            correct_answer = \"Correct answer: \\n\" + element[\"mc1_targets\"][\"choices\"][idx]\n",
    "            text = question + choices + correct_answer\n",
    "            data[\"text\"].append(text)\n",
    "            data[\"tokens\"].append(tokenizer.encode(text))\n",
    "            count += 1\n",
    "\n",
    "            print(f\"\\rget_truthful_qa_multiple_choice(): {i + 1}|{N}, {j + 1}|{M}, {count=}\", end=\"\")\n",
    "            \n",
    "        with open(os.path.join(DATA_PATH, f\"data/{directory_name}/{data_type}_{count:09d}.json\"), \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19796755-2879-4ef7-9196-87745b593c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_orca():\n",
    "    data_type = \"train\"\n",
    "    directory_name = \"open_orca\"\n",
    "    dataset = datasets.load_dataset(\"Open-Orca/OpenOrca\", split=data_type, cache_dir=\"/mnt/data/.cache/huggingface\")\n",
    "\n",
    "    directory = os.path.join(DATA_PATH, f\"data/{directory_name}\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    M = 128\n",
    "    N = len(dataset) // M\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        data = {\"text\": [], \"tokens\": []}\n",
    "        for j in range(M):\n",
    "            element = dataset[count]\n",
    "            system_prompt = \"System prompt: \\n\" + element[\"system_prompt\"] + \"\\n\\n\"\n",
    "            question = \"Question: \\n\" + element[\"question\"] + \"\\n\\n\"\n",
    "            response = \"Response: \\n\" + element[\"response\"] + \"\\n\\n\"\n",
    "            text = system_prompt + question + response\n",
    "            data[\"text\"].append(text)\n",
    "            data[\"tokens\"].append(tokenizer.encode(text))\n",
    "            count += 1\n",
    "\n",
    "            print(f\"\\rget_open_orca(): {i + 1}|{N}, {j + 1}|{M}, {count=}\", end=\"\")\n",
    "            \n",
    "        with open(os.path.join(DATA_PATH, f\"data/{directory_name}/{data_type}_{count:09d}.json\"), \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c1257a-73a0-4b5b-95bd-61cdab9721f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stingning_ultrachat():\n",
    "    data_type = \"train\"\n",
    "    directory_name = \"stingning_ultrachat\"\n",
    "    dataset = datasets.load_dataset(\"stingning/ultrachat\", split=data_type, cache_dir=\"/mnt/data/.cache/huggingface\")\n",
    "\n",
    "    directory = os.path.join(DATA_PATH, f\"data/{directory_name}\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    M = 128\n",
    "    N = len(dataset) // M\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        data = {\"text\": [], \"tokens\": []}\n",
    "        for j in range(M):\n",
    "            element = dataset[count]\n",
    "            question = \"Question: \\n\" + element[\"data\"][0] + \"\\n\\n\"\n",
    "            answer = \"Answer: \\n\" + element[\"data\"][1] + \"\\n\\n\"\n",
    "            text = question + answer\n",
    "            data[\"text\"].append(text)\n",
    "            data[\"tokens\"].append(tokenizer.encode(text))\n",
    "            count += 1\n",
    "\n",
    "            print(f\"\\rget_stingning_ultrachat(): {i + 1}|{N}, {j + 1}|{M}, {count=}\", end=\"\")\n",
    "            \n",
    "        with open(os.path.join(DATA_PATH, f\"data/{directory_name}/{data_type}_{count:09d}.json\"), \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf3c8ca8-4e8e-4737-9a52-b2266d89153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xtreme():\n",
    "    for data_type in [\"validation\", \"test\"]:\n",
    "        directory_name = \"xtreme\"\n",
    "        dataset = datasets.load_dataset(\"xtreme\", \"MLQA.en.en\", split=data_type, cache_dir=\"/mnt/data/.cache/huggingface\")\n",
    "\n",
    "        directory = os.path.join(DATA_PATH, f\"data/{directory_name}\")\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        M = 128\n",
    "        N = len(dataset) // M\n",
    "        count = 0\n",
    "        for i in range(N):\n",
    "            data = {\"text\": [], \"tokens\": []}\n",
    "            for j in range(M):\n",
    "                element = dataset[count]\n",
    "                title = \"Title: \\n\" + element[\"title\"] + \"\\n\\n\"\n",
    "                context = \"Context: \\n\" + element[\"context\"] + \"\\n\\n\"\n",
    "                question = \"Question: \\n\" + element[\"question\"] + \"\\n\\n\"\n",
    "                answer = \"Answer: \\n\" + \"\\n\".join(element[\"answers\"][\"text\"]) + \"\\n\\n\"\n",
    "                text = title + context + question + answer\n",
    "                data[\"text\"].append(text)\n",
    "                data[\"tokens\"].append(tokenizer.encode(text))\n",
    "                count += 1\n",
    "    \n",
    "                print(f\"\\rget_xtreme(): {i + 1}|{N}, {j + 1}|{M}, {count=}\", end=\"\")\n",
    "                \n",
    "            with open(os.path.join(DATA_PATH, f\"data/{directory_name}/{data_type}_{count:09d}.json\"), \"w\") as f:\n",
    "                json.dump(data, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be14806f-4854-4775-8225-2aa8cf95b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openwebtext():\n",
    "    data_type = \"train\"\n",
    "    directory_name = \"openwebtext\"\n",
    "    dataset = datasets.load_dataset(\"Skylion007/openwebtext\", split=data_type, cache_dir=\"/mnt/data/.cache/huggingface\")\n",
    "\n",
    "    directory = os.path.join(DATA_PATH, f\"data/{directory_name}\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    M = 128\n",
    "    N = len(dataset) // M\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        data = {\"text\": [], \"tokens\": []}\n",
    "        for j in range(M):\n",
    "            text = dataset[count][\"text\"]\n",
    "            data[\"text\"].append(text)\n",
    "            data[\"tokens\"].append(tokenizer.encode(text))\n",
    "            count += 1\n",
    "\n",
    "            print(f\"\\rget_openwebtext(): {i + 1}|{N}, {j + 1}|{M}, {count=}\", end=\"\")\n",
    "            \n",
    "        with open(os.path.join(DATA_PATH, f\"data/{directory_name}/{data_type}_{count:09d}.json\"), \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4106468d-bd6d-4dcb-acc9-19fc6317fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oscar_direct_from_source():\n",
    "    print(\"get_oscar_direct_from_source\")\n",
    "\n",
    "    directory = os.path.join(DATA_PATH, f\"data/oscar/text\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    directory = os.path.join(DATA_PATH, f\"data/oscar/tokens\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    _BASE_DATA_URL_FORMAT_STR = (\n",
    "        \"https://s3.amazonaws.com/datasets.huggingface.co/oscar/1.0/{shuffled}/{deduplicated}/{language}/\"\n",
    "    )\n",
    "    _BASE_CHECKSUM_FILE_NAME = \"{language}_sha256.txt\"\n",
    "\n",
    "    language = \"en\"\n",
    "    shuffled_str = \"shuffled\"\n",
    "    deduplicated_str = \"deduplicated\"\n",
    "    base_data_url = _BASE_DATA_URL_FORMAT_STR.format(\n",
    "        shuffled=shuffled_str, language=language, deduplicated=deduplicated_str\n",
    "    )\n",
    "    print(base_data_url)\n",
    "\n",
    "    dl_manager = datasets.DownloadManager()\n",
    "    checksum_url = base_data_url + _BASE_CHECKSUM_FILE_NAME.format(language=language)\n",
    "    print(checksum_url)\n",
    "    #checksum_file = dl_manager.download(checksum_url)\n",
    "    \n",
    "    # with open(checksum_file, encoding=\"utf-8\") as f:\n",
    "    #     data_filenames = [line.split(\"\\t\")[0] for line in f if line]\n",
    "    #     data_urls = [base_url + data_filename for data_filename in data_filenames]\n",
    "    # downloaded_files = dl_manager.download(data_urls)\n",
    "    \n",
    "    # for i, element in enumerate(dataset):\n",
    "    \n",
    "    #     text = element[\"text\"]\n",
    "    #     tokens = tokenizer.encode(text)\n",
    "        \n",
    "    #     with open(os.path.join(DATA_PATH, f\"data/oscar/text/{data_type}_{i:012d}.json\"), \"w\") as f:\n",
    "    #         pass # json.dump({\"text\": text}, f)\n",
    "            \n",
    "    #     with open(os.path.join(DATA_PATH, f\"data/oscar/tokens/{data_type}_{i:012d}.json\"), \"w\") as f:\n",
    "    #         pass # json.dump({\"tokens\": tokens}, f)\n",
    "            \n",
    "    #     if (i + 1) % 5000 == 0:\n",
    "    #         print(f\"{i + 1}|{len(dataset)}\")\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89cfd434-d5b1-47e0-82fb-f0681cdf52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minipile():\n",
    "    data_type = \"train\"\n",
    "    directory_name = \"minipile\"\n",
    "    dataset = datasets.load_dataset(\"JeanKaddour/minipile\", split=data_type)#, cache_dir=\"/mnt/data/.cache/huggingface\")\n",
    "\n",
    "    directory = os.path.join(DATA_PATH, f\"data/{directory_name}\")\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    M = 128\n",
    "    N = len(dataset) // M\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        data = {\"text\": [], \"tokens\": []}\n",
    "        for j in range(M):\n",
    "            text = dataset[count][\"text\"]\n",
    "            data[\"text\"].append(text)\n",
    "            data[\"tokens\"].append(tokenizer.encode(text, allowed_special={'<|endoftext|>'}))\n",
    "            count += 1\n",
    "\n",
    "            print(f\"\\rget_minipile(): {i + 1}|{N}, {j + 1}|{M}, {count=}\", end=\"\")\n",
    "            \n",
    "        with open(os.path.join(DATA_PATH, f\"data/{directory_name}/{data_type}_{count:09d}.json\"), \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca2c5a30-bbd0-4ef6-bbcb-c8dfa816c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cc100_en_from_parquet():\n",
    "    directory_name = \"cc100\"\n",
    "\n",
    "    directory = f\"/mnt/data/{directory_name}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    for j in range(0, 1):\n",
    "        id = f\"{j:04d}\"\n",
    "        key = id + \".parquet\"\n",
    "        filename = os.path.join(\"/mnt/data/parquet\", key)\n",
    "        \n",
    "        table = pq.read_table(filename)\n",
    "        dataset = table.to_pandas()\n",
    "        dataset = np.asarray(dataset)[:, -1]\n",
    "    \n",
    "        M = 128\n",
    "        N = len(dataset) // M\n",
    "        count = 0\n",
    "        for i in range(N):\n",
    "            data = {\"text\": [], \"tokens\": []}\n",
    "            for j in range(M):\n",
    "                text = dataset[count]\n",
    "                data[\"text\"].append(text)\n",
    "                data[\"tokens\"].append(tokenizer.encode(text))\n",
    "                count += 1\n",
    "    \n",
    "                if ((count + 1) % 2500) == 0:\n",
    "                    print(f\"\\rget_cc100_en_from_parquet(): {id} {i + 1}|{N}, {j + 1}|{M}, {count=}\", end=\"\")\n",
    "                \n",
    "            with open(os.path.join(DATA_PATH, f\"data/{directory_name}/{id}_{count:09d}.json\"), \"w\") as f:\n",
    "                json.dump(data, f)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3a4839-0d48-4fd7-ac3a-a3d9b0abb24b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_cc100_en_from_parquet(): 0000 24083|24093, 3|128, count=308249999\n"
     ]
    }
   ],
   "source": [
    "# get_wikipedia()\n",
    "# get_tatsu_lab_alpaca()\n",
    "# get_truthful_qa_generation()\n",
    "# get_truthful_qa_multiple_choice()\n",
    "# get_open_orca()\n",
    "# get_stingning_ultrachat()\n",
    "# get_xtreme()\n",
    "# get_openwebtext()\n",
    "# get_minipile()\n",
    "get_cc100_en_from_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a935d95-4d7e-45c8-a142-498af5b8c7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
